{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, subprocess, requests\n",
    "import pickle\n",
    "from osgeo import gdal\n",
    "import rasterio as rio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "# import georasters as gr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor, ceil\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import neonutilities as nu\n",
    "import geopandas as gpd \n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "from neonutilities.aop_download import validate_dpid,validate_site_format,validate_neon_site\n",
    "from neonutilities.helper_mods.api_helpers import get_api\n",
    "from neonutilities import __resources__\n",
    "from neonutilities.helper_mods.api_helpers import get_api, download_file\n",
    "from neonutilities.helper_mods.metadata_helpers import convert_byte_size\n",
    "from neonutilities.get_issue_log import get_issue_log\n",
    "from neonutilities.citation import get_citation\n",
    "\n",
    "from neonutilities.aop_download import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop and download Vegetation Structure Data (DP1.10098.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_names = ['DELA','LENO','TALL','BONA','DEJU','HEAL','SRER','SJER','SOAP',\n",
    "              'TEAK','CPER','NIWO','RMNP','DSNY','OSBS','JERC','PUUM','KONZ',\n",
    "              'UKFS','SERC','HARV','UNDE','BART','JORN','DCFS','NOGP','WOOD',\n",
    "              'GUAN','LAJA','GRSM','ORNL','CLBJ','MOAB','ONAQ','BLAN','MLBS',\n",
    "              'SCBI','ABBY','WREF','STEI','TREE','YELL']\n",
    "\n",
    "site_worked = []\n",
    "site_failed = []\n",
    "\n",
    "for site_name in site_names:\n",
    "\n",
    "# site_name = site_names[0]\n",
    "    try:\n",
    "        veg_dict = nu.load_by_product(dpid=\"DP1.10098.001\", \n",
    "                                    site=site_name, \n",
    "                                    package=\"basic\", \n",
    "                                    release=\"RELEASE-2025\",\n",
    "                                    check_size=False)\n",
    "\n",
    "        # Save dictionary\n",
    "        with open('/data/chloris/NEON/DP1.10098/' + site_name + '.pkl', 'wb') as f:\n",
    "            pickle.dump(veg_dict, f)\n",
    "        site_worked.append(site_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for site {site_name}: {e}\")\n",
    "        site_failed.append(site_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = os.listdir('/data/chloris/NEON/VST/')\n",
    "flist = [fi for fi in flist if fi.endswith('_single_bole_trees.parquet')]\n",
    "\n",
    "total = 0\n",
    "df_list = []\n",
    "for fname in flist:\n",
    "\n",
    "\tfname = os.path.join('/data/chloris/NEON/VST/', fname)\n",
    "\n",
    "\tdf = pd.read_parquet(fname)\n",
    "\n",
    "\tif df.shape[0] > 0:\n",
    "\t\tdf_list.append(df)\n",
    "\n",
    "\ttotal = total + df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(df_out.individualID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_out.stemDiameter,bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join('/data/chloris/NEON/VST/', flist[0])\n",
    "\n",
    "gpd.read_parquet(fname).to_file('/data/chloris/NEON/DELA.fgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to wrangle veg structure data into a gdf w/ geolocated trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_veg_gdf(veg_dict: dict) -> gpd.GeoDataFrame:\n",
    "\t\"\"\"\n",
    "\tTakes the vegetation structure dict, finds the unique reference points,\n",
    "\tpulls their spatial reference information using the NEON \"locations\" API,\n",
    "\tand computes the location of individual stems.\n",
    "\n",
    "\tNotes:\n",
    "\t1. Stems without a pointID are discarded.\n",
    "\t2. If the points don't all belong to the same utm zone, the function will return a failure.  \n",
    "\n",
    "\tParameters:\n",
    "\tveg_dict (pd.DataFrame): DataFrame containing 'easting' and 'northing' columns.\n",
    "\n",
    "\tReturns:\n",
    "\tThe original dict with the spatial ref information added. \n",
    "\t\"\"\"\n",
    "\n",
    "\t# Pull out the \"mapping and tagging\" dataframe\n",
    "\tveg_map_all = veg_dict[\"vst_mappingandtagging\"]\n",
    "\n",
    "\t# Filter out points that have no pointID and reindex.\n",
    "\tveg_map = veg_map_all.loc[veg_map_all[\"pointID\"] != \"\"]\n",
    "\tveg_map = veg_map.reindex()\n",
    "\n",
    "\t# Create a unique identifier for each point\n",
    "\tveg_map[\"points\"] = veg_map[\"namedLocation\"] + \".\" + veg_map[\"pointID\"]\n",
    "\n",
    "\t# Make a list of the unique points \n",
    "\tveg_points = list(set(list(veg_map[\"points\"])))\n",
    "\n",
    "\t# Loop every point, pull the spatial ref info, and store in lists\n",
    "\teasting = []\n",
    "\tnorthing = []\n",
    "\tcoord_uncertainty = []\n",
    "\telev_uncertainty = []\n",
    "\tutm_zone = []\n",
    "\tfor i in veg_points:\n",
    "\t\tvres = requests.get(\"https://data.neonscience.org/api/v0/locations/\"+i)\n",
    "\t\tvres_json = vres.json()\n",
    "\t\teasting.append(vres_json[\"data\"][\"locationUtmEasting\"])\n",
    "\t\tnorthing.append(vres_json[\"data\"][\"locationUtmNorthing\"])\n",
    "\t\tprops = pd.DataFrame.from_dict(vres_json[\"data\"][\"locationProperties\"])\n",
    "\t\tcu = props.loc[props[\"locationPropertyName\"]==\"Value for Coordinate uncertainty\"][\"locationPropertyValue\"]\n",
    "\t\tcoord_uncertainty.append(cu[cu.index[0]])\n",
    "\t\teu = props.loc[props[\"locationPropertyName\"]==\"Value for Elevation uncertainty\"][\"locationPropertyValue\"]\n",
    "\t\telev_uncertainty.append(eu[eu.index[0]])\n",
    "\t\tutm_zone.append(32600+int(vres_json[\"data\"][\"locationUtmZone\"]))\n",
    "\n",
    "\t# Create a dataframe with the spatial info of the reference points\n",
    "\tpt_dict = dict(points=veg_points, \n",
    "\teasting=easting,\n",
    "\tnorthing=northing,\n",
    "\tcoordinateUncertainty=coord_uncertainty,\n",
    "\televationUncertainty=elev_uncertainty,\n",
    "\tutm_zone = utm_zone)\n",
    "\tpt_df = pd.DataFrame.from_dict(pt_dict)\n",
    "\tpt_df.set_index(\"points\", inplace=True)\n",
    "\n",
    "\t# Add the reference info to the veg map\n",
    "\tveg_map = veg_map.join(pt_df, \n",
    "\ton=\"points\", \n",
    "\thow=\"inner\")\n",
    "\n",
    "\t# Compute the Easting of each stem \n",
    "\tveg_map[\"stemEasting\"] = (veg_map[\"easting\"]\n",
    "\t+ veg_map[\"stemDistance\"]\n",
    "\t* np.sin(veg_map[\"stemAzimuth\"]\n",
    "\t* np.pi / 180))\n",
    "\n",
    "\t# Compute the Northing of each stem\n",
    "\tveg_map[\"stemNorthing\"] = (veg_map[\"northing\"]\n",
    "\t+ veg_map[\"stemDistance\"]\n",
    "\t* np.cos(veg_map[\"stemAzimuth\"]\n",
    "\t* np.pi / 180))\n",
    "\n",
    "\t# Compute the stem uncertainties\n",
    "\tveg_map[\"stemCoordinateUncertainty\"] = veg_map[\"coordinateUncertainty\"] + 0.6\n",
    "\tveg_map[\"stemElevationUncertainty\"] = veg_map[\"elevationUncertainty\"] + 1.5\n",
    "\n",
    "\t# Test that all points are in the same UTM zone\n",
    "\tif len(set(veg_map[\"utm_zone\"])) != 1:\n",
    "\t\traise ValueError(\"Points in the veg map are in different UTM zones! Rectify before proceeding.\")\t\n",
    "\telse:\n",
    "\t\t# Define the crs\n",
    "\t\tcrs = f\"EPSG:{veg_map['utm_zone'].values[0]}\"\n",
    "\t\tprint(f\"veg_map converted to gdf with crs {crs}\")\n",
    "\n",
    "\t\t# Create list of shapely points for the gdf \t\n",
    "\t\tgeometry = [Point(xy) for xy in zip(veg_map[\"stemEasting\"], veg_map[\"stemNorthing\"])]\n",
    "\n",
    "\t\t# Convert veg_map to a gdf \n",
    "\t\tveg_map = gpd.GeoDataFrame(veg_map, crs=crs, geometry=geometry)\n",
    "\n",
    "\t\tveg_dict[\"vst_apparentindividual\"].set_index(\"individualID\", inplace=True)\n",
    "\t\tveg_gdf = veg_map.join(veg_dict[\"vst_apparentindividual\"],\n",
    "\t\ton=\"individualID\",\n",
    "\t\thow=\"inner\",\n",
    "\t\tlsuffix=\"_MAT\",\n",
    "\t\trsuffix=\"_AI\")\n",
    "\n",
    "\t\treturn veg_gdf\n",
    "\n",
    "\n",
    "def filter_veg_gdf(veg_gdf: gpd.GeoDataFrame,\n",
    "\t\t\t\trequire_dbh:bool = True,\n",
    "\t\t\t\tsingle_bole:bool = True,\t\n",
    "\t\t\t\tonly_most_recent:bool = True) -> gpd.GeoDataFrame:\n",
    "\t\"\"\"\n",
    "\tFilters down a vegetation gdf to just the trees, with options to \n",
    "\t1. drop trees without a dbh measurement,\n",
    "\t2. keep only single bole trees,\n",
    "\t3. keep only the most recent measurement of each tree.\n",
    "\n",
    "\tReturns the filtered gdf.\n",
    "\t\"\"\"\n",
    "\n",
    "    # Filter to only trees that have a dbh value\n",
    "\tif require_dbh:\n",
    "\t\tveg_gdf = veg_gdf.loc[~veg_gdf[\"stemDiameter\"].isna()]\t\n",
    "\n",
    "\t# Drop duplicated measurements \n",
    "\tdupe_test_cols = ['date_AI','individualID','scientificName','taxonID','family',\n",
    "\t\t\t\t\t'growthForm','plotID_AI','pointID','stemDiameter',\n",
    "\t\t\t\t\t'maxBaseCrownDiameter','stemEasting','stemNorthing']\n",
    "\tveg_gdf = veg_gdf.drop_duplicates(subset = dupe_test_cols)\n",
    "\n",
    "\t# Cut down to just the tree growth forms \n",
    "\ttree_gdf = veg_gdf[veg_gdf['growthForm'].str.contains('tree|sapling', regex=True)]\n",
    "\n",
    "\t# Cut down to just single bole trees\n",
    "\tif single_bole:\n",
    "\t\ttree_gdf = tree_gdf[(tree_gdf['growthForm']=='single bole tree')]\n",
    "\n",
    "\t# Convert 'date_AI' to datetime if it's not already\n",
    "\ttree_gdf.loc[:, 'date_AI'] = pd.to_datetime(tree_gdf['date_AI'])\n",
    "\n",
    "\t# Sort the DataFrame by 'individualID' and 'date_AI' in descending order\n",
    "\ttree_gdf = tree_gdf.sort_values(by=['individualID', 'date_AI'], ascending=[True, False])\n",
    "\n",
    "\t# Keep only the most recent entry for each 'individualID'\n",
    "\tif only_most_recent:\n",
    "\t\ttree_gdf = tree_gdf.drop_duplicates(subset='individualID', keep='first')\n",
    "\t\t\n",
    "\treturn tree_gdf\n",
    " \n",
    "\n",
    "def nu_list_available_dates(dpid:str, site:str) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "\t\tNOTE: This is an internal hack of the original function to return a dataframe instead of printing\n",
    "        \n",
    "        nu_list_available_dates displays the available releases and dates for a given product and site\n",
    "        --------\n",
    "         Inputs:\n",
    "             dpid: the data product code (eg. 'DP3.30015.001' - CHM)\n",
    "             site: the 4-digit NEON site code (eg. 'JORN')\n",
    "        --------\n",
    "        Returns:\n",
    "        prints the Release Tag (or PROVISIONAL) and the corresponding available dates (YYYY-MM) for each tag\n",
    "    --------\n",
    "        Usage:\n",
    "        --------\n",
    "        >>> list_available_dates('DP3.30015.001','JORN')\n",
    "        RELEASE-2025 Available Dates: 2017-08, 2018-08, 2019-08, 2021-08, 2022-09\n",
    "\n",
    "        >>> list_available_dates('DP3.30015.001','HOPB')\n",
    "        PROVISIONAL Available Dates: 2024-09\n",
    "        RELEASE-2025 Available Dates: 2016-08, 2017-08, 2019-08, 2022-08\n",
    "\n",
    "        >>> list_available_dates('DP1.10098.001','HOPB')\n",
    "        ValueError: There are no data available for the data product DP1.10098.001 at the site HOPB.\n",
    "    \"\"\"\n",
    "    product_url = \"https://data.neonscience.org/api/v0/products/\" + dpid\n",
    "    response = get_api(api_url=product_url)  # add input for token?\n",
    "\n",
    "    # raise value error and print message if dpid isn't formatted as expected\n",
    "    validate_dpid(dpid)\n",
    "\n",
    "    # raise value error and print message if site is not a 4-letter character\n",
    "    site = site.upper()  # make site upper case (if it's not already)\n",
    "    validate_site_format(site)\n",
    "\n",
    "    # raise value error and print message if site is not a valid NEON site\n",
    "    validate_neon_site(site)\n",
    "\n",
    "    # check if product is active\n",
    "    if response.json()[\"data\"][\"productStatus\"] != \"ACTIVE\":\n",
    "        raise ValueError(\n",
    "            f\"NEON {dpid} is not an active data product. See https://data.neonscience.org/data-products/{dpid} for more details.\"\n",
    "        )\n",
    "\n",
    "    # get available releases & months:\n",
    "    for i in range(len(response.json()[\"data\"][\"siteCodes\"])):\n",
    "        if site in response.json()[\"data\"][\"siteCodes\"][i][\"siteCode\"]:\n",
    "            available_releases = response.json()[\"data\"][\"siteCodes\"][i][\n",
    "                \"availableReleases\"\n",
    "            ]\n",
    "\n",
    "    # display available release tags (including provisional) and dates for each tag\n",
    "    try:\n",
    "        availables_list = []\n",
    "        for entry in available_releases:\n",
    "            release = entry[\"release\"]\n",
    "            available_months_str = \", \".join(entry[\"availableMonths\"])\n",
    "            available_months = [x.strip() for x in available_months_str.split(',')]\n",
    "            for available_month in available_months:\n",
    "                availables_list.append({'status':release,'date':available_month})\n",
    "        available_df = pd.DataFrame(availables_list)\n",
    "        return(available_df)\n",
    "    except UnboundLocalError:\n",
    "        # if the available_releases variable doesn't exist, this error will show up:\n",
    "        # UnboundLocalError: local variable 'available_releases' referenced before assignment\n",
    "        raise ValueError(\n",
    "            f\"There are no NEON data available for the data product {dpid} at the site {site}.\"\n",
    "        )\n",
    "    \n",
    "def nu_aop_file_names(\n",
    "\tdpid:str,\n",
    "\tsite:str,\n",
    "\tyear:int,\n",
    "\ttoken=\"\",\n",
    "\tinclude_provisional=True,\n",
    "\tcheck_size=False,\n",
    "\tsavepath=None) -> List:\n",
    "\t\"\"\"returns names of files in the specified AOP data product, site, and year. \n",
    "\t   changes the standard NEON paths to the local paths if savepath specified. \"\"\"\n",
    "\n",
    "\t# raise value error and print message if dpid isn't formatted as expected\n",
    "\tvalidate_dpid(dpid)\n",
    "\n",
    "\t# raise value error and print message if dpid isn't formatted as expected\n",
    "\tvalidate_aop_dpid(dpid)\n",
    "\n",
    "\t# raise value error and print message if field spectra data are attempted\n",
    "\tcheck_field_spectra_dpid(dpid)\n",
    "\n",
    "\t# raise value error and print message if site is not a 4-letter character\n",
    "\tsite = site.upper()  # make site upper case (if it's not already)\n",
    "\tvalidate_site_format(site)\n",
    "\n",
    "\t# raise value error and print message if site is not a valid NEON site\n",
    "\tvalidate_neon_site(site)\n",
    "\n",
    "\t# raise value error and print message if year input is not valid\n",
    "\tyear = str(year)  # cast year to string (if it's not already)\n",
    "\tvalidate_year(year)\n",
    "\n",
    "\t# if token is an empty string, set to None\n",
    "\tif token == \"\":\n",
    "\t\ttoken = None\n",
    "\n",
    "\t# query the products endpoint for the product requested\n",
    "\tresponse = get_api(\"https://data.neonscience.org/api/v0/products/\" + dpid, token)\n",
    "\n",
    "\t# exit function if response is None (eg. if no internet connection)\n",
    "\tif response is None:\n",
    "\t\tlogging.info(\"No response from NEON API. Check internet connection\")\n",
    "\n",
    "\t# check that token was used\n",
    "\tif token and \"x-ratelimit-limit\" in response.headers:\n",
    "\t\tcheck_token(response)\n",
    "\t\t# if response.headers['x-ratelimit-limit'] == '200':\n",
    "\t\t#     print('API token was not recognized. Public rate limit applied.\\n')\n",
    "\n",
    "\t# get the request response dictionary\n",
    "\tresponse_dict = response.json()\n",
    "\n",
    "\t# error message if dpid is not an AOP data product\n",
    "\tcheck_aop_dpid(response_dict, dpid)\n",
    "\n",
    "\t# replace collocated site with the AOP site name it's published under\n",
    "\tsite = get_shared_flights(site)\n",
    "\n",
    "\t# get the urls for months with data available, and subset to site & year\n",
    "\tsite_year_urls = get_site_year_urls(response_dict, site, year)\n",
    "\n",
    "\t# error message if nothing is available\n",
    "\tif len(site_year_urls) == 0:\n",
    "\t\tlogging.info(\n",
    "\t\t\tf\"There are no NEON {dpid} data available at the site {site} in {year}.\\nTo display available dates for a given data product and site, use the function list_available_dates().\"\n",
    "\t\t)\n",
    "\t\t# print(\"There are no data available at the selected site and year.\")\n",
    "\n",
    "\t# get file url dataframe for the available month urls\n",
    "\tfile_url_df, releases = get_file_urls(site_year_urls, token=token)\n",
    "\n",
    "\t# get the number of files in the dataframe, if there are no files to download, return\n",
    "\tif len(file_url_df) == 0:\n",
    "\t\t# print(\"No data files found.\")\n",
    "\t\tlogging.info(\"No NEON data files found.\")\n",
    "\t\t# return\n",
    "\n",
    "\t# NOTE: provisional filtering has been silenced for now. \n",
    "\t# if 'PROVISIONAL' in releases and not include_provisional:\n",
    "\t# if include_provisional:\n",
    "\t# \t# log provisional included message\n",
    "\t# \t# logging.info(\n",
    "\t# \t# \t\"Provisional NEON data are included. To exclude provisional data, use input parameter include_provisional=False.\"\n",
    "\t# \t# )\n",
    "\t# else:\n",
    "\t# \t# log provisional not included message and filter to the released data\n",
    "\t# \t# logging.info(\n",
    "\t# \t#     \"Provisional data are not included. To download provisional data, use input parameter include_provisional=True.\")\n",
    "\t# \tfile_url_df = file_url_df[file_url_df[\"release\"] != \"PROVISIONAL\"]\n",
    "\t# \tif len(file_url_df) == 0:\n",
    "\t# \t\tlogging.info(\n",
    "\t# \t\t\t\"NEON Provisional data are not included. To download provisional data, use input parameter include_provisional=True.\"\n",
    "\t# \t\t)\n",
    "\n",
    "\tnum_files = len(file_url_df)\n",
    "\tif num_files == 0:\n",
    "\t\tlogging.info(\n",
    "\t\t\t\"No NEON data files found. Available data may all be provisional. To download provisional data, use input parameter include_provisional=True.\"\n",
    "\t\t)\n",
    "\t\t# return\n",
    "\n",
    "\t# get the total size of all the files found\n",
    "\tdownload_size_bytes = file_url_df[\"size\"].sum()\n",
    "\t# print(f'download size, bytes: {download_size_bytes}')\n",
    "\tdownload_size = convert_byte_size(download_size_bytes)\n",
    "\t# print(f'download size: {download_size}')\n",
    "\n",
    "\t# report data download size and ask user if they want to proceed\n",
    "\tif check_size:\n",
    "\t\tif (\n",
    "\t\t\tinput(\n",
    "\t\t\t\tf\"Continuing will download {num_files} NEON data files totaling approximately {download_size}. Do you want to proceed? (y/n) \"\n",
    "\t\t\t)\n",
    "\t\t\t.strip()\n",
    "\t\t\t.lower()\n",
    "\t\t\t!= \"y\"\n",
    "\t\t):  # lower or upper case 'y' will work\n",
    "\t\t\tprint(\"Download halted.\")\n",
    "\t\t\t# return\n",
    "\n",
    "\t# Make the list of files as they should appear on the local file system and return \n",
    "\tfiles = list(file_url_df[\"url\"])\n",
    "\tif savepath is not None:\n",
    "\t\tfiles = [f\"{savepath.rstrip('/')}/{fi.lstrip('https://storage.googleapis.com')}\" for fi in files]\n",
    "\treturn files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpid = \"DP3.30015.001\"\n",
    "site = \"DELA\"\n",
    "year = 2017\n",
    "dp3_path = '/data/chloris/NEON/DP3.30015.001/'\n",
    "\n",
    "out_path = '/data/chloris/NEON/CHM/'\n",
    "\n",
    "site_names = ['DELA','LENO','TALL','BONA','DEJU','HEAL','SRER','SJER','SOAP',\n",
    "              'TEAK','CPER','NIWO','RMNP','DSNY','OSBS','JERC','PUUM','KONZ',\n",
    "              'UKFS','SERC','HARV','UNDE','BART','JORN','DCFS',]\n",
    " \n",
    "site_names = ['NOGP','WOOD','GUAN','LAJA','GRSM','ORNL','CLBJ','MOAB','ONAQ','BLAN','MLBS','SCBI','ABBY','WREF','STEI','TREE','YELL']\n",
    "\n",
    "for site in site_names:\n",
    "\n",
    "\t# Get available dates \n",
    "\tavailable_dates_df = nu_list_available_dates(dpid=dpid, site=site)\n",
    "\n",
    "\t# Parse out the years\n",
    "\tavailable_dates_df['year'] = available_dates_df['date'].str.split('-').str[0].astype(int)\n",
    "\n",
    "\t# Loop the years\n",
    "\tfor year in available_dates_df['year'].unique():\n",
    "\n",
    "\t\t# Figure out the output name\n",
    "\t\tout_fname = f\"{out_path.rstrip('/')}/{site}_{year}_CHM.tif\"\n",
    "\n",
    "\t\tprint(f'Merging {site} {year}')\n",
    "\n",
    "\t\t# Get the list of files for this site and year \n",
    "\t\tflist = nu_aop_file_names(\n",
    "\t\t\tdpid = dpid,\n",
    "\t\t\tsite = site,\n",
    "\t\t\tyear = year,\n",
    "\t\t\tsavepath=dp3_path)\n",
    "\n",
    "\t\t# Grab the tifs only\n",
    "\t\ttif_list = [fi for fi in flist if fi.endswith('.tif')]\n",
    "\n",
    "\t\t# Write the file names to a text file\n",
    "\t\ttif_list_file = f\"{out_path.rstrip('/')}/file_lists/{site}_{year}_tif_list.txt\"\n",
    "\t\twith open(tif_list_file, \"w\") as f:\n",
    "\t\t\tfor tif in tif_list:\n",
    "\t\t\t\tf.write(f\"{tif}\\n\")\n",
    "\n",
    "\t\t#Construct the gdal string \n",
    "\t\tgdal_cmd = (\n",
    "\t\t\tf\"gdalwarp --optfile {tif_list_file} -co TILED=YES \"\n",
    "\t\t\tf\"-co BLOCKXSIZE=1024 -co BLOCKYSIZE=1024 {out_fname}\"\n",
    "\t\t\tf\" -co COMPRESS=LZW -co PREDICTOR=2\"\n",
    "\t\t)\n",
    "\n",
    "\t\tsubprocess.run(gdal_cmd, shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canopy Height Model Data (CHM - DP3.30015.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import importlib_resources\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "# local imports\n",
    "from neonutilities import __resources__\n",
    "from neonutilities.helper_mods.api_helpers import get_api, download_file\n",
    "from neonutilities.helper_mods.metadata_helpers import convert_byte_size\n",
    "from neonutilities.get_issue_log import get_issue_log\n",
    "from neonutilities.citation import get_citation\n",
    "\n",
    "from neonutilities.aop_download import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most recent data is from 2024, so we can use that. This is still available provisionally  (eg. not as part of a NEON data release), because everything generated in 2024 is provisional in 2025 (there is a year lag period). The data should be good as long as there are no reported issues in the Issue Logs (see the Data Portal). Next we can see the extent of the data. You don't need to run this step, but it can be helpful for context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sjer_exts = nu.get_aop_tile_extents('DP3.30015.001','SJER','2024')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Eastings and Northings determined from the vegetation data to download only the tiles that overlay with the veg data points. Use `by_tile_aop` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sjer_exts = nu.get_aop_tile_extents('DP3.30015.001','SJER','2024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sjer_exts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.by_tile_aop(dpid=\"DP3.30015.001\", site=\"SJER\", year=\"2024\", \n",
    "          easting=list(trees_sb_latest.adjEasting), \n",
    "          northing=list(trees_sb_latest.adjNorthing),\n",
    "          include_provisional=True,\n",
    "          savepath=os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what files were downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm_files = []\n",
    "for root, dirs, files in os.walk(\"DP3.30015.001\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".tif\"):\n",
    "            print(file)\n",
    "            chm_files.append(os.path.join(root,file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify, you can pare down the data again to only look at the central rectangular region. Those are the following 6 tiles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save this merged CHM raster to a new geotiff file, called `SJER_2024_CHM_merged.tif`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update metadata for the merged CHM \n",
    "output_meta = chm.meta.copy()\n",
    "output_meta.update({\n",
    "    \"driver\": \"GTiff\",\n",
    "    \"height\": chm_mosaic.shape[1],\n",
    "    \"width\": chm_mosaic.shape[2],\n",
    "    \"transform\": chm_transform,\n",
    "})\n",
    "\n",
    "# Write the merged raster to a new GeoTIFF file\n",
    "output_path = 'SJER_2024_CHM_merged.tif'\n",
    "with rio.open(output_path, \"w\", **output_meta) as dest:\n",
    "    dest.write(chm_mosaic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now filter the TOS dataset to only include the values inside this smaller extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may decide to keep only the Live trees, depending on your appication. As you can see, there are other categories, including Standing dead; Live, physically damaged; and Live, disease damaged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all the trees - can show the taxonID and growthForms, or other information you are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'hue' argument to color by species\n",
    "sns.lmplot(x=\"adjEasting\", y=\"adjNorthing\", data=veg, fit_reg=False, hue='taxonID', legend=False);\n",
    "# anchor the legend to outside right of the plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "# Use the 'hue' argument to color by species\n",
    "sns.lmplot(x=\"adjEasting\", y=\"adjNorthing\", data=veg, fit_reg=False, hue='growthForm', legend=False);\n",
    "# anchor the legend to outside right of the plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are links to each of the USDA plant profiles. \n",
    "\n",
    "USDA Plant Profiles:\n",
    "- https://plants.usda.gov/core/profile?symbol=qudo - Quercus douglasii (blue oak)\n",
    "- https://plants.usda.gov/core/profile?symbol=cecu - Ceanothus cuneatus (buckbrush)\n",
    "- https://plants.usda.gov/core/profile?symbol=QUWI2 - Quercus wislizeni (interior live oak)\n",
    "- https://plants.usda.gov/core/profile?symbol=PISA2 - Pinus sabiniana (california foothill pine)\n",
    "- https://plants.usda.gov/core/profile?symbol=CELE2 - Ceanothus leucodermis (chaparral whitethorn) *shrub\n",
    "- https://plants.usda.gov/core/profile?symbol=LUAL4 - Lupinus albifrons (silver lupine) *shrub\n",
    "- https://plants.usda.gov/core/profile?symbol=ARVIM - Arctostaphylos viscida (Mariposa manzanita) *shrub tree\n",
    "- https://plants.usda.gov/core/profile?symbol=RHIL - Rhamnus ilicifolia (hollyleaf redberry) *shrub tree\n",
    "\n",
    "Plot only the single bole trees (`trees_sb_subset`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"adjEasting\", y=\"adjNorthing\", data=trees_sb_subset, fit_reg=False, hue='taxonID', legend=False);\n",
    "# anchor the legend to outside right of the plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n",
    "plt.title('Single Bole Trees'); plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the single-bole trees are Oaks (QUDO), (QUWI2). \n",
    "\n",
    "Next, extract the CHM values corresponding to each of the veg data points. You can do this in several different ways and with several different Python packages. We will show using `rasterio` (imported as `rio`) as well as `georasters`, which provides more advanced extraction options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in with rasterio\n",
    "chm_merged = rio.open(\"SJER_2024_CHM_merged.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the CHM value corresponding to each of the trees in the TOS single-bole tree dataset (trees_sb_subset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valCHM = list(rio.sample.sample_gen(chm_merged, \n",
    "                                tuple(zip(trees_sb_subset[\"adjEasting\"], \n",
    "                                          trees_sb_subset[\"adjNorthing\"])),\n",
    "                                masked=True))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot((0,50), (0,50), linewidth=1, color=\"black\")\n",
    "ax.scatter(trees_sb_subset.height, valCHM, s=1.5)\n",
    "\n",
    "ax.set_xlabel(\"TOS Height\")\n",
    "ax.set_ylabel(\"CHM Height\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is fairly good alignment between the CHM heights and TOS heights, with the exception of a few outliers. Keep in mind that anything above the 1-1 line (CHM height > TOS height) might represent understory, as the CHM represents the canopy height. There may be other reasons for outliers - including geographic mismatch. Say the point locations between the TOS and Lidar-derived positions are off by a meter, the lidar could be detecting ground where the TOS data includes a tree. In the plot above we can see several examples where the CHM height is 0 and the TOS height is > 5 m. You may need to either adjust point locations, or filter out these outliers to exclude them from the model. \n",
    "\n",
    "You can also look at the stem diameter v. CHM height. There should be a positive correlation, but more scatter is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot of stemDiameter v. CHM height\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(trees_sb_subset.stemDiameter, valCHM, s=1.5)\n",
    "\n",
    "ax.set_xlabel(\"TOS Stem Diameter (cm)\")\n",
    "ax.set_ylabel(\"CHM Height (m)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your approach, you may need to filter out outliers and/or remove the understory plants. Remember the CHM represents the canopy trees, so can mask the understory. See the Veg Structure and CHM lesson (https://www.neonscience.org/resources/learning-hub/tutorials/tree-heights-veg-structure-chm) for some ideas on further filtering.\n",
    "\n",
    "We can also make a function for plotting the rasters, this is optional but another way to display the CHM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raster(band_array,image_extent,title,cmap_title,colormap,colormap_limits):\n",
    "    plt.imshow(band_array,extent=image_extent)\n",
    "    cbar = plt.colorbar(); plt.set_cmap(colormap); plt.clim(colormap_limits)\n",
    "    cbar.set_label(cmap_title,rotation=270,labelpad=20)\n",
    "    plt.title(title); ax = plt.gca()\n",
    "    ax.ticklabel_format(useOffset=False, style='plain') \n",
    "    rotatexlabels = plt.setp(ax.get_xticklabels(),rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install georasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import georasters as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Python `georasters` package (imported as `gr`) to read in the mosaicked raster. This gives us a little more flexibility for extraction (eg. we can select the max CHM value within a given radius)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sjer_chm = gr.from_file('SJER_2024_CHM_merged.tif')\n",
    "sjer_chm_array=sjer_chm.raster.data\n",
    "sjer_chm_extent=[sjer_chm.bounds[0],sjer_chm.bounds[2],sjer_chm.bounds[1],sjer_chm.bounds[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_raster(sjer_chm_array,sjer_chm_extent,'SJER CHM Subset','Canopy Height (m)','Greens',[0,30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select the CHM height, and the maximum CHM height within a 3 m radius. This may help with cases where there is geographic mismatch between the TOS and CHM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_training_df = trees_sb_subset[(np.isfinite(trees_sb_subset['adjEasting'])) & (np.isfinite(trees_sb_subset['adjNorthing']))].reset_index(drop=True)\n",
    "for i,row in tree_training_df.iterrows():\n",
    "    chm_height = sjer_chm.map_pixel(tree_training_df.loc[i,'adjEasting'],tree_training_df.loc[i,'adjNorthing'])  \n",
    "    chm_height_max3 = sjer_chm.extract(tree_training_df.loc[i,'adjEasting'],tree_training_df.loc[i,'adjNorthing'],radius=3).max()\n",
    "    tree_training_df.at[i,'chm_height'] = chm_height\n",
    "    tree_training_df.at[i,'chm_height_max3'] = chm_height_max3\n",
    "tree_training_df[['date_AI','individualID','scientificName','taxonID','family','growthForm','plantStatus','stemDiameter','height','chm_height','chm_height_max3']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see with the 3rd record (`NEON.PLA.D17.SJER.00037`), the `chm_height` is 0, but the `chm_height_max3` is 6.18, much closer to the TOS measured height of 6.5. This may be a better variable to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot TOS v. CHM heights for comparison, colored by taxonID:\n",
    "sns.lmplot(tree_training_df, x='height', y='chm_height', hue='taxonID', fit_reg=False); ax = plt.gca()\n",
    "#plot 1-1 line for comparison\n",
    "compare_line = np.arange(0,ceil(np.nanmax([tree_training_df['height'],tree_training_df['chm_height_max3']])))\n",
    "ax.plot(compare_line,compare_line,'--',color='red')\n",
    "ax.axis('equal')\n",
    "ax.set_title('AOP v. TOS Tree Heights, CHM Pixel Height')\n",
    "ax.set_xlabel('TOS Height (m)')\n",
    "ax.set_ylabel('AOP CHM Height (m)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one Oak with an anomalously tall height of 57 m. We can look at this more closely to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_training_df[tree_training_df['height']>50][['height','chm_height','chm_height_max3','stemDiameter','dataQF_AI']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legacy data was collected before data collection app (Fulcrum) was available, so are subject to more human/recorder error. This may have been off by a decimal place. While the stem diameter seems reasonable, you may choose to remove this record, as we do not expect any Oak trees this tall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_training_df = tree_training_df[tree_training_df['height']<50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CHM heights tend to be lower than field-measured heights, which fits with this plot. We interpret values above the 1-1 line to be understory trees, which we can filter out according to some threshold. Let's try a ratio of 0.75 to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_filtered = tree_training_df.loc[(tree_training_df['height'] > 0.75*tree_training_df['chm_height']) & (tree_training_df['chm_height']>2)]\n",
    "sns.lmplot(training_df_filtered, x='height', y='chm_height_max3', hue='taxonID', fit_reg=False);\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also wish to remove the PISA2 outlier, where the TOS height is ~ 15m but the chm_height_max3 < 5m. This may be due to geographic mismatch, or some other manual error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_filtered[(training_df_filtered['chm_height_max3']<5) & (training_df_filtered['taxonID'] == 'PISA2')][['individualID','taxonID','height','chm_height','chm_height_max3','stemDiameter']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be off due to a geographic mismatch, or some other error. Remove this record as well, to be safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_filtered = training_df_filtered[training_df_filtered['individualID'] != 'NEON.PLA.D17.SJER.04443']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biomass calculations - allometric equations\n",
    "\n",
    "Now that we've identified and filtered out the outliers, let's calculate biomass for the filtered dataframe. For multi-bole trees, calculate biomass for each bole according to the standard formula, and then aggregate by `individualID`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USDA Plant Profiles:**\n",
    "- https://plants.usda.gov/core/profile?symbol=qudo - Quercus douglasii (blue oak)\n",
    "- https://plants.usda.gov/core/profile?symbol=QUWI2 - Quercus wislizeni (interior live oak)\n",
    "- https://plants.usda.gov/core/profile?symbol=PISA2 - Pinus sabiniana (california foothill pine)\n",
    "\n",
    "**Biomass equation:**\n",
    "\n",
    "$Biomass = exp(\\beta_0 + \\beta_1 ln(DBH))$\n",
    "\n",
    "where\n",
    "- Biomass = total aboveground biomass (kg) for trees 2.5cm dbh and larger\n",
    "- DBH = diameter at breast height (cm)\n",
    "\n",
    "Make a dataframe containing the allometry parameters. We suggest using the following references to obtain these values:\n",
    "\n",
    "Updated generalized biomass equations for North American tree species.\n",
    "David C. Chojnacky, Linda S. Heath, Jennifer C. Jenkins 2013\n",
    "https://academic.oup.com/forestry/article-abstract/87/1/129/602137?redirectedFrom=fulltext\n",
    "\n",
    "Comprehensive database of diameter-based biomass regressions for North American tree species. \n",
    "Jennifer C. Jenkins, David C. Chojnacky, Linda S. Heath, Richard A. Birdsey 2004\n",
    "https://research.fs.usda.gov/treesearch/7058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allometry_params_df = pd.DataFrame(columns=['speciesGroup','taxonID','B0','B1'])\n",
    "allometry_params_df.loc[len(allometry_params_df)] = ['Hard maple/oak/hickory/beech','QUDO',-2.0127,2.4342]\n",
    "allometry_params_df.loc[len(allometry_params_df)] = ['Hard maple/oak/hickory/beech','QUWI2',-2.0127,2.4342]\n",
    "allometry_params_df.loc[len(allometry_params_df)] = ['Pine','PISA2',-2.5356,2.4349]\n",
    "allometry_params_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the allometry parameters, we can write a function to calculate biomass and apply this to the training dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_biomass(B0,B1,DBH):\n",
    "    biomass = np.exp(B0+B1*np.log(DBH))\n",
    "    return biomass\n",
    "\n",
    "for i,row in tree_training_df.iterrows():\n",
    "    #find allometry params corresponding to taxonID\n",
    "    allometry_params = allometry_params_df.loc[(allometry_params_df['taxonID'] == tree_training_df.loc[i,'taxonID'])]\n",
    "    B0 = allometry_params['B0'].item(); #print(B0)\n",
    "    B1 = allometry_params['B1'].item(); #print(B1)\n",
    "    DBH = tree_training_df.loc[i,'stemDiameter']\n",
    "    tree_training_df.at[i,'biomass'] = calc_biomass(B0,B1,DBH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the training dataframe to a csv. You can select fewer or more columns as desired. You can expand upon this by adding region properties from the CHM data. This will be expanded upon in a later iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_df = tree_training_df[['date_AI','individualID','adjEasting','adjNorthing','taxonID','growthForm','plantStatus','stemDiameter','height','chm_height_max3','chm_height','biomass']]\n",
    "final_training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_df.to_csv('tree_training_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use this df as a trianing dataset. You may choose to further filter by only including Live trees, or "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
