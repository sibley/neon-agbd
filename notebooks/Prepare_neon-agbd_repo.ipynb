{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('/home/yew/code/neon-agbd/data/NEONForestAGB/NEONForestAGBv2.csv')\n",
    "df.shape\n",
    "\n",
    "# Figure out how to break df down into 10 parts by rows, saving each part as a csv\n",
    "num_parts = 10\n",
    "rows_per_part = df.shape[0] // num_parts\n",
    "\n",
    "for i in range(num_parts):\n",
    "    start_row = i * rows_per_part\n",
    "    if i == num_parts - 1:\n",
    "        end_row = df.shape[0]\n",
    "    else:\n",
    "        end_row = (i + 1) * rows_per_part\n",
    "    part_df = df.iloc[start_row:end_row]\n",
    "    part_df.to_csv(f'/home/yew/code/neon-agbd/data/NEONForestAGB/NEONForestAGBv2_part{i+1:02}.csv', index=False)\n",
    "    print(f'Saved part {i+1} with rows {start_row} to {end_row}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, subprocess, requests\n",
    "import pickle\n",
    "from osgeo import gdal\n",
    "import rasterio as rio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "# import georasters as gr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor, ceil\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import neonutilities as nu\n",
    "import geopandas as gpd \n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "from neonutilities.aop_download import validate_dpid,validate_site_format,validate_neon_site\n",
    "from neonutilities.helper_mods.api_helpers import get_api\n",
    "from neonutilities import __resources__\n",
    "from neonutilities.helper_mods.api_helpers import get_api, download_file\n",
    "from neonutilities.helper_mods.metadata_helpers import convert_byte_size\n",
    "from neonutilities.get_issue_log import get_issue_log\n",
    "from neonutilities.citation import get_citation\n",
    "\n",
    "from neonutilities.aop_download import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpids = ['DELA','LENO','TALL','BONA','DEJU','HEAL','SRER','SJER','SOAP',\n",
    "              'TEAK','CPER','NIWO','RMNP','DSNY','OSBS','JERC','PUUM','KONZ',\n",
    "              'UKFS','SERC','HARV','UNDE','BART','JORN','DCFS','NOGP','WOOD',\n",
    "              'GUAN','LAJA','GRSM','ORNL','CLBJ','MOAB','ONAQ','MLBS',\n",
    "              'SCBI','ABBY','WREF','TREE','YELL'] # STEI, BLAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_name = 'DELA'\n",
    "\n",
    "# Read the site pickle\n",
    "with open('/home/yew/code/neon-agbd/data/DP1.10098/' + site_name + '.pkl', 'rb') as f:\n",
    "    veg_dict = pickle.load(f)\n",
    "\n",
    "veg_dict.keys()\n",
    "\n",
    "#veg_dict['vst_perplotperyear'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_dict['vst_perplotperyear']#['growthForm'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_dict['vst_apparentindividual']['growthForm'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_dict['vst_apparentindividual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_veg_gdf(veg_dict: dict) -> gpd.GeoDataFrame:\n",
    "\t\"\"\"\n",
    "\tTakes the vegetation structure dict, finds the unique reference points,\n",
    "\tpulls their spatial reference information using the NEON \"locations\" API,\n",
    "\tand computes the location of individual stems.\n",
    "\n",
    "\tNotes:\n",
    "\t1. Stems without a pointID are discarded.\n",
    "\t2. If the points don't all belong to the same utm zone, the function will return a failure.  \n",
    "\t3. If the georeferencing point cannot be found via the API, all associated stems will be discarded. \n",
    "\n",
    "\tParameters:\n",
    "\tveg_dict (pd.DataFrame): DataFrame containing 'easting' and 'northing' columns.\n",
    "\n",
    "\tReturns:\n",
    "\tThe original dict with the spatial ref information added. \n",
    "\t\"\"\"\n",
    "\n",
    "\t# Pull out the \"mapping and tagging\" dataframe\n",
    "\tveg_map_all = veg_dict[\"vst_mappingandtagging\"]\n",
    "\n",
    "\t# Filter out points that have no pointID and reindex.\n",
    "\tveg_map = veg_map_all.loc[veg_map_all[\"pointID\"] != \"\"]\n",
    "\tveg_map = veg_map.reindex()\n",
    "\n",
    "\t# Create a unique identifier for each point\n",
    "\tveg_map[\"points\"] = veg_map[\"namedLocation\"] + \".\" + veg_map[\"pointID\"]\n",
    "\n",
    "\t# Make a list of the unique points \n",
    "\tveg_points = list(set(list(veg_map[\"points\"])))\n",
    "\n",
    "\t# Loop every point, pull the spatial ref info, and store in lists\n",
    "\tvalid_points = []\n",
    "\teasting = []\n",
    "\tnorthing = []\n",
    "\tcoord_uncertainty = []\n",
    "\telev_uncertainty = []\n",
    "\tutm_zone = []\n",
    "\tfor i in veg_points:\n",
    "\t\t# vres = requests.get(\"https://data.neonscience.org/api/v0/locations/\"+i)\n",
    "\t\tvres = session.get(\"https://data.neonscience.org/api/v0/locations/\"+i)\n",
    "\t\tvres_json = vres.json()\n",
    "\t\tif not vres_json.get(\"data\"):\n",
    "\t\t\tcontinue\n",
    "\t\tvalid_points.append(i)\n",
    "\t\teasting.append(vres_json[\"data\"][\"locationUtmEasting\"])\n",
    "\t\tnorthing.append(vres_json[\"data\"][\"locationUtmNorthing\"])\n",
    "\t\tprops = pd.DataFrame.from_dict(vres_json[\"data\"][\"locationProperties\"])\n",
    "\t\tcu = props.loc[props[\"locationPropertyName\"]==\"Value for Coordinate uncertainty\"][\"locationPropertyValue\"]\n",
    "\t\tif cu.empty:\n",
    "\t\t\tcu = np.nan\n",
    "\t\telse: \n",
    "\t\t\tcu = cu[cu.index[0]]\n",
    "\t\tcoord_uncertainty.append(cu)\t\n",
    "\t\teu = props.loc[props[\"locationPropertyName\"]==\"Value for Elevation uncertainty\"][\"locationPropertyValue\"]\n",
    "\t\tif eu.empty:\n",
    "\t\t\t# eu = pd.Series([np.nan])\n",
    "\t\t\teu = np.nan\n",
    "\t\telse:\n",
    "\t\t\teu = eu[eu.index[0]]\n",
    "\t\telev_uncertainty.append(eu)\n",
    "\t\tutm_zone.append(32600+int(vres_json[\"data\"][\"locationUtmZone\"]))\n",
    "\n",
    "\t# Create a dataframe with the spatial info of the reference points\n",
    "\tpt_dict = dict(points=valid_points, \n",
    "\teasting=easting,\n",
    "\tnorthing=northing,\n",
    "\tcoordinateUncertainty=coord_uncertainty,\n",
    "\televationUncertainty=elev_uncertainty,\n",
    "\tutm_zone = utm_zone)\n",
    "\n",
    "\tpt_df = pd.DataFrame.from_dict(pt_dict)\n",
    "\tpt_df.set_index(\"points\", inplace=True)\n",
    "\n",
    "\t# Add the reference info to the veg map\n",
    "\tveg_map = veg_map.join(pt_df, \n",
    "\ton=\"points\", \n",
    "\thow=\"inner\")\n",
    "\n",
    "\t# Compute the Easting of each stem \n",
    "\tveg_map[\"stemEasting\"] = (veg_map[\"easting\"]\n",
    "\t+ veg_map[\"stemDistance\"]\n",
    "\t* np.sin(veg_map[\"stemAzimuth\"]\n",
    "\t* np.pi / 180))\n",
    "\n",
    "\t# Compute the Northing of each stem\n",
    "\tveg_map[\"stemNorthing\"] = (veg_map[\"northing\"]\n",
    "\t+ veg_map[\"stemDistance\"]\n",
    "\t* np.cos(veg_map[\"stemAzimuth\"]\n",
    "\t* np.pi / 180))\n",
    "\n",
    "\t# Compute the stem uncertainties\n",
    "\tveg_map[\"stemCoordinateUncertainty\"] = veg_map[\"coordinateUncertainty\"] + 0.6\n",
    "\tveg_map[\"stemElevationUncertainty\"] = veg_map[\"elevationUncertainty\"] + 1.5\n",
    "\n",
    "\t# Test that all points are in the same UTM zone\n",
    "\tif len(set(veg_map[\"utm_zone\"])) != 1:\n",
    "\t\traise ValueError(\"Points in the veg map are in different UTM zones! Rectify before proceeding.\")\t\n",
    "\telse:\n",
    "\t\t# Define the crs\n",
    "\t\tcrs = f\"EPSG:{veg_map['utm_zone'].values[0]}\"\n",
    "\t\tprint(f\"veg_map converted to gdf with crs {crs}\")\n",
    "\n",
    "\t\t# Create list of shapely points for the gdf \t\n",
    "\t\tgeometry = [Point(xy) for xy in zip(veg_map[\"stemEasting\"], veg_map[\"stemNorthing\"])]\n",
    "\n",
    "\t\t# Convert veg_map to a gdf \n",
    "\t\tveg_map = gpd.GeoDataFrame(veg_map, crs=crs, geometry=geometry)\n",
    "\n",
    "\t\tveg_dict[\"vst_apparentindividual\"].set_index(\"individualID\", inplace=True)\n",
    "\t\tveg_gdf = veg_map.join(veg_dict[\"vst_apparentindividual\"],\n",
    "\t\ton=\"individualID\",\n",
    "\t\thow=\"inner\",\n",
    "\t\tlsuffix=\"_MAT\",\n",
    "\t\trsuffix=\"_AI\")\n",
    "\n",
    "\t\treturn veg_gdf\n",
    "\n",
    "\n",
    "def filter_veg_gdf(veg_gdf: gpd.GeoDataFrame,\n",
    "\t\t\t\trequire_dbh:bool = True,\n",
    "\t\t\t\tsingle_bole:bool = True,\t\n",
    "\t\t\t\tonly_most_recent:bool = True) -> gpd.GeoDataFrame:\n",
    "\t\"\"\"\n",
    "\tFilters down a vegetation gdf to just the trees, with options to \n",
    "\t1. drop trees without a dbh measurement,\n",
    "\t2. keep only single bole trees,\n",
    "\t3. keep only the most recent measurement of each tree.\n",
    "\n",
    "\tReturns the filtered gdf.\n",
    "\t\"\"\"\n",
    "\n",
    "    # Filter to only trees that have a dbh value\n",
    "\tif require_dbh:\n",
    "\t\tveg_gdf = veg_gdf.loc[~veg_gdf[\"stemDiameter\"].isna()]\t\n",
    "\n",
    "\t# Drop duplicated measurements \n",
    "\tdupe_test_cols = ['date_AI','individualID','scientificName','taxonID','family',\n",
    "\t\t\t\t\t'growthForm','plotID_AI','pointID','stemDiameter',\n",
    "\t\t\t\t\t'maxBaseCrownDiameter','stemEasting','stemNorthing']\n",
    "\tveg_gdf = veg_gdf.drop_duplicates(subset = dupe_test_cols)\n",
    "\n",
    "\t# Cut down to just the tree growth forms \n",
    "\ttree_gdf = veg_gdf[veg_gdf['growthForm'].str.contains('tree|sapling', regex=True)]\n",
    "\n",
    "\t# Cut down to just single bole trees\n",
    "\tif single_bole:\n",
    "\t\ttree_gdf = tree_gdf[(tree_gdf['growthForm']=='single bole tree')]\n",
    "\n",
    "\t# Convert 'date_AI' to datetime if it's not already\n",
    "\ttree_gdf.loc[:, 'date_AI'] = pd.to_datetime(tree_gdf['date_AI'])\n",
    "\n",
    "\t# Sort the DataFrame by 'individualID' and 'date_AI' in descending order\n",
    "\ttree_gdf = tree_gdf.sort_values(by=['individualID', 'date_AI'], ascending=[True, False])\n",
    "\n",
    "\t# Keep only the most recent entry for each 'individualID'\n",
    "\tif only_most_recent:\n",
    "\t\ttree_gdf = tree_gdf.drop_duplicates(subset='individualID', keep='first')\n",
    "\t\t\n",
    "\treturn tree_gdf\n",
    " \n",
    "\n",
    "def nu_list_available_dates(dpid:str, site:str) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "\t\tNOTE: This is an internal hack of the original function to return a dataframe instead of printing\n",
    "        \n",
    "        nu_list_available_dates displays the available releases and dates for a given product and site\n",
    "        --------\n",
    "         Inputs:\n",
    "             dpid: the data product code (eg. 'DP3.30015.001' - CHM)\n",
    "             site: the 4-digit NEON site code (eg. 'JORN')\n",
    "        --------\n",
    "        Returns:\n",
    "        prints the Release Tag (or PROVISIONAL) and the corresponding available dates (YYYY-MM) for each tag\n",
    "    --------\n",
    "        Usage:\n",
    "        --------\n",
    "        >>> list_available_dates('DP3.30015.001','JORN')\n",
    "        RELEASE-2025 Available Dates: 2017-08, 2018-08, 2019-08, 2021-08, 2022-09\n",
    "\n",
    "        >>> list_available_dates('DP3.30015.001','HOPB')\n",
    "        PROVISIONAL Available Dates: 2024-09\n",
    "        RELEASE-2025 Available Dates: 2016-08, 2017-08, 2019-08, 2022-08\n",
    "\n",
    "        >>> list_available_dates('DP1.10098.001','HOPB')\n",
    "        ValueError: There are no data available for the data product DP1.10098.001 at the site HOPB.\n",
    "    \"\"\"\n",
    "    product_url = \"https://data.neonscience.org/api/v0/products/\" + dpid\n",
    "    response = get_api(api_url=product_url)  # add input for token?\n",
    "\n",
    "    # raise value error and print message if dpid isn't formatted as expected\n",
    "    validate_dpid(dpid)\n",
    "\n",
    "    # raise value error and print message if site is not a 4-letter character\n",
    "    site = site.upper()  # make site upper case (if it's not already)\n",
    "    validate_site_format(site)\n",
    "\n",
    "    # raise value error and print message if site is not a valid NEON site\n",
    "    validate_neon_site(site)\n",
    "\n",
    "    # check if product is active\n",
    "    if response.json()[\"data\"][\"productStatus\"] != \"ACTIVE\":\n",
    "        raise ValueError(\n",
    "            f\"NEON {dpid} is not an active data product. See https://data.neonscience.org/data-products/{dpid} for more details.\"\n",
    "        )\n",
    "\n",
    "    # get available releases & months:\n",
    "    for i in range(len(response.json()[\"data\"][\"siteCodes\"])):\n",
    "        if site in response.json()[\"data\"][\"siteCodes\"][i][\"siteCode\"]:\n",
    "            available_releases = response.json()[\"data\"][\"siteCodes\"][i][\n",
    "                \"availableReleases\"\n",
    "            ]\n",
    "\n",
    "    # display available release tags (including provisional) and dates for each tag\n",
    "    try:\n",
    "        availables_list = []\n",
    "        for entry in available_releases:\n",
    "            release = entry[\"release\"]\n",
    "            available_months_str = \", \".join(entry[\"availableMonths\"])\n",
    "            available_months = [x.strip() for x in available_months_str.split(',')]\n",
    "            for available_month in available_months:\n",
    "                availables_list.append({'status':release,'date':available_month})\n",
    "        available_df = pd.DataFrame(availables_list)\n",
    "        return(available_df)\n",
    "    except UnboundLocalError:\n",
    "        # if the available_releases variable doesn't exist, this error will show up:\n",
    "        # UnboundLocalError: local variable 'available_releases' referenced before assignment\n",
    "        raise ValueError(\n",
    "            f\"There are no NEON data available for the data product {dpid} at the site {site}.\"\n",
    "        )\n",
    "    \n",
    "def nu_aop_file_names(\n",
    "\tdpid:str,\n",
    "\tsite:str,\n",
    "\tyear:int,\n",
    "\ttoken=\"\",\n",
    "\tinclude_provisional=True,\n",
    "\tcheck_size=False,\n",
    "\tsavepath=None) -> List:\n",
    "\t\"\"\"returns names of files in the specified AOP data product, site, and year. \n",
    "\t   changes the standard NEON paths to the local paths if savepath specified. \"\"\"\n",
    "\n",
    "\t# raise value error and print message if dpid isn't formatted as expected\n",
    "\tvalidate_dpid(dpid)\n",
    "\n",
    "\t# raise value error and print message if dpid isn't formatted as expected\n",
    "\tvalidate_aop_dpid(dpid)\n",
    "\n",
    "\t# raise value error and print message if field spectra data are attempted\n",
    "\tcheck_field_spectra_dpid(dpid)\n",
    "\n",
    "\t# raise value error and print message if site is not a 4-letter character\n",
    "\tsite = site.upper()  # make site upper case (if it's not already)\n",
    "\tvalidate_site_format(site)\n",
    "\n",
    "\t# raise value error and print message if site is not a valid NEON site\n",
    "\tvalidate_neon_site(site)\n",
    "\n",
    "\t# raise value error and print message if year input is not valid\n",
    "\tyear = str(year)  # cast year to string (if it's not already)\n",
    "\tvalidate_year(year)\n",
    "\n",
    "\t# if token is an empty string, set to None\n",
    "\tif token == \"\":\n",
    "\t\ttoken = None\n",
    "\n",
    "\t# query the products endpoint for the product requested\n",
    "\tresponse = get_api(\"https://data.neonscience.org/api/v0/products/\" + dpid, token)\n",
    "\n",
    "\t# exit function if response is None (eg. if no internet connection)\n",
    "\tif response is None:\n",
    "\t\tlogging.info(\"No response from NEON API. Check internet connection\")\n",
    "\n",
    "\t# check that token was used\n",
    "\tif token and \"x-ratelimit-limit\" in response.headers:\n",
    "\t\tcheck_token(response)\n",
    "\t\t# if response.headers['x-ratelimit-limit'] == '200':\n",
    "\t\t#     print('API token was not recognized. Public rate limit applied.\\n')\n",
    "\n",
    "\t# get the request response dictionary\n",
    "\tresponse_dict = response.json()\n",
    "\n",
    "\t# error message if dpid is not an AOP data product\n",
    "\tcheck_aop_dpid(response_dict, dpid)\n",
    "\n",
    "\t# replace collocated site with the AOP site name it's published under\n",
    "\tsite = get_shared_flights(site)\n",
    "\n",
    "\t# get the urls for months with data available, and subset to site & year\n",
    "\tsite_year_urls = get_site_year_urls(response_dict, site, year)\n",
    "\n",
    "\t# error message if nothing is available\n",
    "\tif len(site_year_urls) == 0:\n",
    "\t\tlogging.info(\n",
    "\t\t\tf\"There are no NEON {dpid} data available at the site {site} in {year}.\\nTo display available dates for a given data product and site, use the function list_available_dates().\"\n",
    "\t\t)\n",
    "\t\t# print(\"There are no data available at the selected site and year.\")\n",
    "\n",
    "\t# get file url dataframe for the available month urls\n",
    "\tfile_url_df, releases = get_file_urls(site_year_urls, token=token)\n",
    "\n",
    "\t# get the number of files in the dataframe, if there are no files to download, return\n",
    "\tif len(file_url_df) == 0:\n",
    "\t\t# print(\"No data files found.\")\n",
    "\t\tlogging.info(\"No NEON data files found.\")\n",
    "\t\t# return\n",
    "\n",
    "\t# NOTE: provisional filtering has been silenced for now. \n",
    "\t# if 'PROVISIONAL' in releases and not include_provisional:\n",
    "\t# if include_provisional:\n",
    "\t# \t# log provisional included message\n",
    "\t# \t# logging.info(\n",
    "\t# \t# \t\"Provisional NEON data are included. To exclude provisional data, use input parameter include_provisional=False.\"\n",
    "\t# \t# )\n",
    "\t# else:\n",
    "\t# \t# log provisional not included message and filter to the released data\n",
    "\t# \t# logging.info(\n",
    "\t# \t#     \"Provisional data are not included. To download provisional data, use input parameter include_provisional=True.\")\n",
    "\t# \tfile_url_df = file_url_df[file_url_df[\"release\"] != \"PROVISIONAL\"]\n",
    "\t# \tif len(file_url_df) == 0:\n",
    "\t# \t\tlogging.info(\n",
    "\t# \t\t\t\"NEON Provisional data are not included. To download provisional data, use input parameter include_provisional=True.\"\n",
    "\t# \t\t)\n",
    "\n",
    "\tnum_files = len(file_url_df)\n",
    "\tif num_files == 0:\n",
    "\t\tlogging.info(\n",
    "\t\t\t\"No NEON data files found. Available data may all be provisional. To download provisional data, use input parameter include_provisional=True.\"\n",
    "\t\t)\n",
    "\t\t# return\n",
    "\n",
    "\t# get the total size of all the files found\n",
    "\tdownload_size_bytes = file_url_df[\"size\"].sum()\n",
    "\t# print(f'download size, bytes: {download_size_bytes}')\n",
    "\tdownload_size = convert_byte_size(download_size_bytes)\n",
    "\t# print(f'download size: {download_size}')\n",
    "\n",
    "\t# report data download size and ask user if they want to proceed\n",
    "\tif check_size:\n",
    "\t\tif (\n",
    "\t\t\tinput(\n",
    "\t\t\t\tf\"Continuing will download {num_files} NEON data files totaling approximately {download_size}. Do you want to proceed? (y/n) \"\n",
    "\t\t\t)\n",
    "\t\t\t.strip()\n",
    "\t\t\t.lower()\n",
    "\t\t\t!= \"y\"\n",
    "\t\t):  # lower or upper case 'y' will work\n",
    "\t\t\tprint(\"Download halted.\")\n",
    "\t\t\t# return\n",
    "\n",
    "\t# Make the list of files as they should appear on the local file system and return \n",
    "\tfiles = list(file_url_df[\"url\"])\n",
    "\tif savepath is not None:\n",
    "\t\tfiles = [f\"{savepath.rstrip('/')}/{fi.lstrip('https://storage.googleapis.com')}\" for fi in files]\n",
    "\treturn files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the NEON veg data files through to gdfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpids = ['DELA','LENO','TALL','BONA','DEJU','HEAL','SRER','SJER','SOAP',\n",
    "              'TEAK','CPER','NIWO','RMNP','DSNY','OSBS','JERC','PUUM','KONZ',\n",
    "              'UKFS','SERC','HARV','UNDE','BART','JORN','DCFS','NOGP','WOOD',\n",
    "              'GUAN','LAJA','GRSM','ORNL','CLBJ','MOAB','ONAQ','MLBS',\n",
    "              'SCBI','ABBY','WREF','TREE','YELL'] # STEI, BLAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Set up a session with retries\n",
    "session = requests.Session()\n",
    "retries = Retry(\n",
    "    total=5,                # Total retry attempts\n",
    "    backoff_factor=0.5,     # Wait time between retries: 0.5, 1, 2, 4, etc.\n",
    "    status_forcelist=[500, 502, 503, 504],  # Retry on these HTTP status codes\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "skipped_list = ['BONA','SOAP','TEAK','NIWO','BLAN','STEI'] # BLAN and STEI has UTM zone issues! 2 different zones. Rerun MLBS too!\n",
    "\n",
    "rerun = ['BONA','SOAP','TEAK','NIWO','MLBS']\n",
    "\n",
    "site_name = site_names[0]\n",
    "\n",
    "succeeded_list = []\n",
    "\n",
    "# for i, site_name in enumerate(site_names):\n",
    "for i, site_name in enumerate(site_names):\n",
    "\n",
    "\n",
    "\tif i <= -1:\n",
    "\t\tcontinue\n",
    "\n",
    "\tstart_timer = time.time()\n",
    "\n",
    "\t# Read the site pickle\n",
    "\twith open('/data/chloris/NEON/DP1.10098/' + site_name + '.pkl', 'rb') as f:\n",
    "\t\tveg_dict = pickle.load(f)\n",
    "\n",
    "\t# Make the veg gdf \n",
    "\tveg_gdf = make_veg_gdf(veg_dict)\n",
    "\n",
    "\t# Write that out \n",
    "\tveg_gdf.to_parquet('/data/chloris/NEON/VST/' + site_name + '_all.parquet')\n",
    "\n",
    "\t# Make the filtered veg gdf\n",
    "\ttree_gdf = filter_veg_gdf(veg_gdf,only_most_recent=False)\n",
    "\n",
    "\t# Write that out\n",
    "\ttree_gdf.to_parquet('/data/chloris/NEON/VST/' + site_name + '_single_bole_trees.parquet')\n",
    "\n",
    "\tsucceeded_list.append(site_name)\n",
    "\tprint(f\"Finished {i} - {site_name} in {time.time() - start_timer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_name\n",
    "\n",
    "# # Read the site pickle\n",
    "# with open('/data/chloris/NEON/DP1.10098/' + site_name + '.pkl', 'rb') as f:\n",
    "# \tveg_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the \"mapping and tagging\" dataframe\n",
    "veg_map_all = veg_dict[\"vst_mappingandtagging\"]\n",
    "\n",
    "# Filter out points that have no pointID and reindex.\n",
    "veg_map = veg_map_all.loc[veg_map_all[\"pointID\"] != \"\"]\n",
    "veg_map = veg_map.reindex()\n",
    "\n",
    "# Create a unique identifier for each point\n",
    "veg_map[\"points\"] = veg_map[\"namedLocation\"] + \".\" + veg_map[\"pointID\"]\n",
    "\n",
    "# Make a list of the unique points \n",
    "veg_points = list(set(list(veg_map[\"points\"])))\n",
    "\n",
    "# Loop every point, pull the spatial ref info, and store in lists\n",
    "valid_points = []\n",
    "easting = []\n",
    "northing = []\n",
    "coord_uncertainty = []\n",
    "elev_uncertainty = []\n",
    "utm_zone = []\n",
    "for i in veg_points:\n",
    "\t# vres = requests.get(\"https://data.neonscience.org/api/v0/locations/\"+i)\n",
    "\tvres = session.get(\"https://data.neonscience.org/api/v0/locations/\"+i)\n",
    "\tvres_json = vres.json()\n",
    "\tif not vres_json.get(\"data\"):\n",
    "\t\tcontinue\n",
    "\tvalid_points.append(i)\n",
    "\teasting.append(vres_json[\"data\"][\"locationUtmEasting\"])\n",
    "\tnorthing.append(vres_json[\"data\"][\"locationUtmNorthing\"])\n",
    "\tprops = pd.DataFrame.from_dict(vres_json[\"data\"][\"locationProperties\"])\n",
    "\tcu = props.loc[props[\"locationPropertyName\"]==\"Value for Coordinate uncertainty\"][\"locationPropertyValue\"]\n",
    "\tif cu.empty:\n",
    "\t\t# cu = pd.Series([np.nan])\n",
    "\t\tcu = np.nan\n",
    "\telse: \n",
    "\t\tcu = cu[cu.index[0]]\n",
    "\tcoord_uncertainty.append(cu)\t\n",
    "\teu = props.loc[props[\"locationPropertyName\"]==\"Value for Elevation uncertainty\"][\"locationPropertyValue\"]\n",
    "\tif eu.empty:\n",
    "\t\t# eu = pd.Series([np.nan])\n",
    "\t\teu = np.nan\n",
    "\telse:\n",
    "\t\teu = eu[eu.index[0]]\n",
    "\telev_uncertainty.append(eu)\n",
    "\tutm_zone.append(32600+int(vres_json[\"data\"][\"locationUtmZone\"]))\n",
    "\n",
    "# Create a dataframe with the spatial info of the reference points\n",
    "pt_dict = dict(points=valid_points, \n",
    "easting=easting,\n",
    "northing=northing,\n",
    "coordinateUncertainty=coord_uncertainty,\n",
    "elevationUncertainty=elev_uncertainty,\n",
    "utm_zone = utm_zone)\n",
    "\n",
    "pt_df = pd.DataFrame.from_dict(pt_dict)\n",
    "pt_df.set_index(\"points\", inplace=True)\n",
    "\n",
    "# Add the reference info to the veg map\n",
    "veg_map = veg_map.join(pt_df, \n",
    "on=\"points\", \n",
    "how=\"inner\")\n",
    "\n",
    "# Compute the Easting of each stem \n",
    "veg_map[\"stemEasting\"] = (veg_map[\"easting\"]\n",
    "+ veg_map[\"stemDistance\"]\n",
    "* np.sin(veg_map[\"stemAzimuth\"]\n",
    "* np.pi / 180))\n",
    "\n",
    "# Compute the Northing of each stem\n",
    "veg_map[\"stemNorthing\"] = (veg_map[\"northing\"]\n",
    "+ veg_map[\"stemDistance\"]\n",
    "* np.cos(veg_map[\"stemAzimuth\"]\n",
    "* np.pi / 180))\n",
    "\n",
    "# Compute the stem uncertainties\n",
    "veg_map[\"stemCoordinateUncertainty\"] = veg_map[\"coordinateUncertainty\"] + 0.6\n",
    "veg_map[\"stemElevationUncertainty\"] = veg_map[\"elevationUncertainty\"] + 1.5\n",
    "\n",
    "# Test that all points are in the same UTM zone\n",
    "if len(set(veg_map[\"utm_zone\"])) != 1:\n",
    "\traise ValueError(\"Points in the veg map are in different UTM zones! Rectify before proceeding.\")\t\n",
    "else:\n",
    "\t# Define the crs\n",
    "\tcrs = f\"EPSG:{veg_map['utm_zone'].values[0]}\"\n",
    "\tprint(f\"veg_map converted to gdf with crs {crs}\")\n",
    "\n",
    "\t# Create list of shapely points for the gdf \t\n",
    "\tgeometry = [Point(xy) for xy in zip(veg_map[\"stemEasting\"], veg_map[\"stemNorthing\"])]\n",
    "\n",
    "\t# Convert veg_map to a gdf \n",
    "\tveg_map = gpd.GeoDataFrame(veg_map, crs=crs, geometry=geometry)\n",
    "\n",
    "\tveg_dict[\"vst_apparentindividual\"].set_index(\"individualID\", inplace=True)\n",
    "\tveg_gdf = veg_map.join(veg_dict[\"vst_apparentindividual\"],\n",
    "\ton=\"individualID\",\n",
    "\thow=\"inner\",\n",
    "\tlsuffix=\"_MAT\",\n",
    "\trsuffix=\"_AI\")\n",
    "\n",
    "\t# return veg_gdf\n",
    "\tveg_gdf.to_parquet('/data/chloris/NEON/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(veg_gdf['coordinateUncertainty'].isna())\n",
    "# veg_gdf.to_parquet('/data/chloris/NEON/test.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chloris-agbd-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
