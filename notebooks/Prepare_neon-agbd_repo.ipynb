{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation succeeded 2026-01-07 14:02:03.612100\n"
     ]
    }
   ],
   "source": [
    "branch = \"dev\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import toml\n",
    "import subprocess\n",
    "import warnings\n",
    "import dask\n",
    "from datetime import datetime\n",
    "from dask_gateway import Gateway\n",
    "\n",
    "gitlab_token = toml.load(os.path.join(Path.home(), \".gitlab_token\"))\n",
    "# gitlab_token = toml.load(.gitlab_token\"))\n",
    "\n",
    "proc = subprocess.Popen(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\"]\n",
    "    + [\n",
    "        \"--no-dependencies\",  # the docker image already has all agbd deps installed\n",
    "        \"--upgrade\",  # override the version already in the image (may not be needed)\n",
    "        \"--force-reinstall\",  # always reinstall\n",
    "        \"--no-cache-dir\",  # disable caching\n",
    "        f\"\"\"git+https://{gitlab_token['TOKEN_NAME']}:{gitlab_token['ACCESS_TOKEN']}@gitlab.com/chloris-geospatial/data-science/chloris-agbd.git@{branch}\"\"\",\n",
    "    ],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    ")\n",
    "stdout, stderr = proc.communicate()\n",
    "returncode = proc.wait()\n",
    "if returncode:\n",
    "    raise Exception(\"Installation failed!\", stderr)\n",
    "print(f\"Installation succeeded {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/yew/code/neon-agbd/data/NEONForestAGB/NEONForestAGBv2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/yew/code/neon-agbd/data/NEONForestAGB/NEONForestAGBv2.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m df.shape\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Figure out how to break df down into 10 parts by rows, saving each part as a csv\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chloris-agbd-base/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chloris-agbd-base/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chloris-agbd-base/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chloris-agbd-base/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chloris-agbd-base/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/yew/code/neon-agbd/data/NEONForestAGB/NEONForestAGBv2.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('/home/yew/code/neon-agbd/data/NEONForestAGB/NEONForestAGBv2.csv')\n",
    "df.shape\n",
    "\n",
    "# Figure out how to break df down into 10 parts by rows, saving each part as a csv\n",
    "num_parts = 10\n",
    "rows_per_part = df.shape[0] // num_parts\n",
    "\n",
    "for i in range(num_parts):\n",
    "    start_row = i * rows_per_part\n",
    "    if i == num_parts - 1:\n",
    "        end_row = df.shape[0]\n",
    "    else:\n",
    "        end_row = (i + 1) * rows_per_part\n",
    "    part_df = df.iloc[start_row:end_row]\n",
    "    part_df.to_csv(f'/home/yew/code/neon-agbd/data/NEONForestAGB/NEONForestAGBv2_part{i+1:02}.csv', index=False)\n",
    "    print(f'Saved part {i+1} with rows {start_row} to {end_row}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, subprocess, requests\n",
    "import pickle\n",
    "from osgeo import gdal\n",
    "import rasterio as rio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "# import georasters as gr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor, ceil\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import neonutilities as nu\n",
    "import geopandas as gpd \n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "from neonutilities.aop_download import validate_dpid,validate_site_format,validate_neon_site\n",
    "from neonutilities.helper_mods.api_helpers import get_api\n",
    "from neonutilities import __resources__\n",
    "from neonutilities.helper_mods.api_helpers import get_api, download_file\n",
    "from neonutilities.helper_mods.metadata_helpers import convert_byte_size\n",
    "from neonutilities.get_issue_log import get_issue_log\n",
    "from neonutilities.citation import get_citation\n",
    "\n",
    "from neonutilities.aop_download import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpids = ['DELA','LENO','TALL','BONA','DEJU','HEAL','SRER','SJER','SOAP',\n",
    "              'TEAK','CPER','NIWO','RMNP','DSNY','OSBS','JERC','PUUM','KONZ',\n",
    "              'UKFS','SERC','HARV','UNDE','BART','JORN','DCFS','NOGP','WOOD',\n",
    "              'GUAN','LAJA','GRSM','ORNL','CLBJ','MOAB','ONAQ','MLBS',\n",
    "              'SCBI','ABBY','WREF','TREE','YELL'] # STEI, BLAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uid', 'namedLocation', 'date', 'nonwoodyCollectDate', 'domainID',\n",
       "       'siteID', 'plotID', 'plotType', 'nlcdClass', 'decimalLatitude',\n",
       "       'decimalLongitude', 'geodeticDatum', 'coordinateUncertainty', 'easting',\n",
       "       'northing', 'utmZone', 'elevation', 'elevationUncertainty',\n",
       "       'samplingImpractical', 'eventID', 'eventType', 'dataCollected',\n",
       "       'subplotsSampled', 'samplingProtocolVersion', 'targetTaxaPresent',\n",
       "       'woodyTargetTaxaPresent', 'nonWoodyTargetTaxaPresent', 'treesPresent',\n",
       "       'shrubsPresent', 'lianasPresent', 'cactiPresent', 'fernsPresent',\n",
       "       'yuccasPresent', 'palmsPresent', 'ocotillosPresent',\n",
       "       'xerophyllumPresent', 'treeFernsPresent',\n",
       "       'nestedSubplotAreaShrubSapling', 'nestedSubplotAreaLiana',\n",
       "       'nestedSubplotAreaFerns', 'nestedSubplotAreaOther',\n",
       "       'totalSampledAreaTrees', 'totalSampledAreaShrubSapling',\n",
       "       'totalSampledAreaLiana', 'totalSampledAreaFerns',\n",
       "       'totalSampledAreaOther', 'remarks', 'measuredBy', 'recordedBy',\n",
       "       'dataQF', 'publicationDate', 'release'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_name = 'DELA'\n",
    "\n",
    "# Read the site pickle\n",
    "with open('/home/yew/code/neon-agbd/data/DP1.10098/' + site_name + '.pkl', 'rb') as f:\n",
    "    veg_dict = pickle.load(f)\n",
    "\n",
    "veg_dict.keys()\n",
    "\n",
    "veg_dict['vst_perplotperyear'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['single bole tree', 'multi-bole tree', 'small tree', 'liana',\n",
       "       'sapling', 'single shrub', 'small shrub', ''], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veg_dict['vst_apparentindividual']['growthForm'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['single bole tree', 'multi-bole tree', 'small tree', 'liana',\n",
       "       'sapling', 'single shrub', 'small shrub', ''], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veg_dict['vst_apparentindividual']['growthForm'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['categoricalCodes_10098', 'citation_10098_RELEASE-2025', 'issueLog_10098', 'readme_10098', 'validation_10098', 'variables_10098', 'vst_apparentindividual', 'vst_mappingandtagging', 'vst_perplotperyear'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veg_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_veg_gdf(veg_dict: dict) -> gpd.GeoDataFrame:\n",
    "\t\"\"\"\n",
    "\tTakes the vegetation structure dict, finds the unique reference points,\n",
    "\tpulls their spatial reference information using the NEON \"locations\" API,\n",
    "\tand computes the location of individual stems.\n",
    "\n",
    "\tNotes:\n",
    "\t1. Stems without a pointID are discarded.\n",
    "\t2. If the points don't all belong to the same utm zone, the function will return a failure.  \n",
    "\t3. If the georeferencing point cannot be found via the API, all associated stems will be discarded. \n",
    "\n",
    "\tParameters:\n",
    "\tveg_dict (pd.DataFrame): DataFrame containing 'easting' and 'northing' columns.\n",
    "\n",
    "\tReturns:\n",
    "\tThe original dict with the spatial ref information added. \n",
    "\t\"\"\"\n",
    "\n",
    "\t# Pull out the \"mapping and tagging\" dataframe\n",
    "\tveg_map_all = veg_dict[\"vst_mappingandtagging\"]\n",
    "\n",
    "\t# Filter out points that have no pointID and reindex.\n",
    "\tveg_map = veg_map_all.loc[veg_map_all[\"pointID\"] != \"\"]\n",
    "\tveg_map = veg_map.reindex()\n",
    "\n",
    "\t# Create a unique identifier for each point\n",
    "\tveg_map[\"points\"] = veg_map[\"namedLocation\"] + \".\" + veg_map[\"pointID\"]\n",
    "\n",
    "\t# Make a list of the unique points \n",
    "\tveg_points = list(set(list(veg_map[\"points\"])))\n",
    "\n",
    "\t# Loop every point, pull the spatial ref info, and store in lists\n",
    "\tvalid_points = []\n",
    "\teasting = []\n",
    "\tnorthing = []\n",
    "\tcoord_uncertainty = []\n",
    "\telev_uncertainty = []\n",
    "\tutm_zone = []\n",
    "\tfor i in veg_points:\n",
    "\t\t# vres = requests.get(\"https://data.neonscience.org/api/v0/locations/\"+i)\n",
    "\t\tvres = session.get(\"https://data.neonscience.org/api/v0/locations/\"+i)\n",
    "\t\tvres_json = vres.json()\n",
    "\t\tif not vres_json.get(\"data\"):\n",
    "\t\t\tcontinue\n",
    "\t\tvalid_points.append(i)\n",
    "\t\teasting.append(vres_json[\"data\"][\"locationUtmEasting\"])\n",
    "\t\tnorthing.append(vres_json[\"data\"][\"locationUtmNorthing\"])\n",
    "\t\tprops = pd.DataFrame.from_dict(vres_json[\"data\"][\"locationProperties\"])\n",
    "\t\tcu = props.loc[props[\"locationPropertyName\"]==\"Value for Coordinate uncertainty\"][\"locationPropertyValue\"]\n",
    "\t\tif cu.empty:\n",
    "\t\t\tcu = np.nan\n",
    "\t\telse: \n",
    "\t\t\tcu = cu[cu.index[0]]\n",
    "\t\tcoord_uncertainty.append(cu)\t\n",
    "\t\teu = props.loc[props[\"locationPropertyName\"]==\"Value for Elevation uncertainty\"][\"locationPropertyValue\"]\n",
    "\t\tif eu.empty:\n",
    "\t\t\t# eu = pd.Series([np.nan])\n",
    "\t\t\teu = np.nan\n",
    "\t\telse:\n",
    "\t\t\teu = eu[eu.index[0]]\n",
    "\t\telev_uncertainty.append(eu)\n",
    "\t\tutm_zone.append(32600+int(vres_json[\"data\"][\"locationUtmZone\"]))\n",
    "\n",
    "\t# Create a dataframe with the spatial info of the reference points\n",
    "\tpt_dict = dict(points=valid_points, \n",
    "\teasting=easting,\n",
    "\tnorthing=northing,\n",
    "\tcoordinateUncertainty=coord_uncertainty,\n",
    "\televationUncertainty=elev_uncertainty,\n",
    "\tutm_zone = utm_zone)\n",
    "\n",
    "\tpt_df = pd.DataFrame.from_dict(pt_dict)\n",
    "\tpt_df.set_index(\"points\", inplace=True)\n",
    "\n",
    "\t# Add the reference info to the veg map\n",
    "\tveg_map = veg_map.join(pt_df, \n",
    "\ton=\"points\", \n",
    "\thow=\"inner\")\n",
    "\n",
    "\t# Compute the Easting of each stem \n",
    "\tveg_map[\"stemEasting\"] = (veg_map[\"easting\"]\n",
    "\t+ veg_map[\"stemDistance\"]\n",
    "\t* np.sin(veg_map[\"stemAzimuth\"]\n",
    "\t* np.pi / 180))\n",
    "\n",
    "\t# Compute the Northing of each stem\n",
    "\tveg_map[\"stemNorthing\"] = (veg_map[\"northing\"]\n",
    "\t+ veg_map[\"stemDistance\"]\n",
    "\t* np.cos(veg_map[\"stemAzimuth\"]\n",
    "\t* np.pi / 180))\n",
    "\n",
    "\t# Compute the stem uncertainties\n",
    "\tveg_map[\"stemCoordinateUncertainty\"] = veg_map[\"coordinateUncertainty\"] + 0.6\n",
    "\tveg_map[\"stemElevationUncertainty\"] = veg_map[\"elevationUncertainty\"] + 1.5\n",
    "\n",
    "\t# Test that all points are in the same UTM zone\n",
    "\tif len(set(veg_map[\"utm_zone\"])) != 1:\n",
    "\t\traise ValueError(\"Points in the veg map are in different UTM zones! Rectify before proceeding.\")\t\n",
    "\telse:\n",
    "\t\t# Define the crs\n",
    "\t\tcrs = f\"EPSG:{veg_map['utm_zone'].values[0]}\"\n",
    "\t\tprint(f\"veg_map converted to gdf with crs {crs}\")\n",
    "\n",
    "\t\t# Create list of shapely points for the gdf \t\n",
    "\t\tgeometry = [Point(xy) for xy in zip(veg_map[\"stemEasting\"], veg_map[\"stemNorthing\"])]\n",
    "\n",
    "\t\t# Convert veg_map to a gdf \n",
    "\t\tveg_map = gpd.GeoDataFrame(veg_map, crs=crs, geometry=geometry)\n",
    "\n",
    "\t\tveg_dict[\"vst_apparentindividual\"].set_index(\"individualID\", inplace=True)\n",
    "\t\tveg_gdf = veg_map.join(veg_dict[\"vst_apparentindividual\"],\n",
    "\t\ton=\"individualID\",\n",
    "\t\thow=\"inner\",\n",
    "\t\tlsuffix=\"_MAT\",\n",
    "\t\trsuffix=\"_AI\")\n",
    "\n",
    "\t\treturn veg_gdf\n",
    "\n",
    "\n",
    "def filter_veg_gdf(veg_gdf: gpd.GeoDataFrame,\n",
    "\t\t\t\trequire_dbh:bool = True,\n",
    "\t\t\t\tsingle_bole:bool = True,\t\n",
    "\t\t\t\tonly_most_recent:bool = True) -> gpd.GeoDataFrame:\n",
    "\t\"\"\"\n",
    "\tFilters down a vegetation gdf to just the trees, with options to \n",
    "\t1. drop trees without a dbh measurement,\n",
    "\t2. keep only single bole trees,\n",
    "\t3. keep only the most recent measurement of each tree.\n",
    "\n",
    "\tReturns the filtered gdf.\n",
    "\t\"\"\"\n",
    "\n",
    "    # Filter to only trees that have a dbh value\n",
    "\tif require_dbh:\n",
    "\t\tveg_gdf = veg_gdf.loc[~veg_gdf[\"stemDiameter\"].isna()]\t\n",
    "\n",
    "\t# Drop duplicated measurements \n",
    "\tdupe_test_cols = ['date_AI','individualID','scientificName','taxonID','family',\n",
    "\t\t\t\t\t'growthForm','plotID_AI','pointID','stemDiameter',\n",
    "\t\t\t\t\t'maxBaseCrownDiameter','stemEasting','stemNorthing']\n",
    "\tveg_gdf = veg_gdf.drop_duplicates(subset = dupe_test_cols)\n",
    "\n",
    "\t# Cut down to just the tree growth forms \n",
    "\ttree_gdf = veg_gdf[veg_gdf['growthForm'].str.contains('tree|sapling', regex=True)]\n",
    "\n",
    "\t# Cut down to just single bole trees\n",
    "\tif single_bole:\n",
    "\t\ttree_gdf = tree_gdf[(tree_gdf['growthForm']=='single bole tree')]\n",
    "\n",
    "\t# Convert 'date_AI' to datetime if it's not already\n",
    "\ttree_gdf.loc[:, 'date_AI'] = pd.to_datetime(tree_gdf['date_AI'])\n",
    "\n",
    "\t# Sort the DataFrame by 'individualID' and 'date_AI' in descending order\n",
    "\ttree_gdf = tree_gdf.sort_values(by=['individualID', 'date_AI'], ascending=[True, False])\n",
    "\n",
    "\t# Keep only the most recent entry for each 'individualID'\n",
    "\tif only_most_recent:\n",
    "\t\ttree_gdf = tree_gdf.drop_duplicates(subset='individualID', keep='first')\n",
    "\t\t\n",
    "\treturn tree_gdf\n",
    " \n",
    "\n",
    "def nu_list_available_dates(dpid:str, site:str) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "\t\tNOTE: This is an internal hack of the original function to return a dataframe instead of printing\n",
    "        \n",
    "        nu_list_available_dates displays the available releases and dates for a given product and site\n",
    "        --------\n",
    "         Inputs:\n",
    "             dpid: the data product code (eg. 'DP3.30015.001' - CHM)\n",
    "             site: the 4-digit NEON site code (eg. 'JORN')\n",
    "        --------\n",
    "        Returns:\n",
    "        prints the Release Tag (or PROVISIONAL) and the corresponding available dates (YYYY-MM) for each tag\n",
    "    --------\n",
    "        Usage:\n",
    "        --------\n",
    "        >>> list_available_dates('DP3.30015.001','JORN')\n",
    "        RELEASE-2025 Available Dates: 2017-08, 2018-08, 2019-08, 2021-08, 2022-09\n",
    "\n",
    "        >>> list_available_dates('DP3.30015.001','HOPB')\n",
    "        PROVISIONAL Available Dates: 2024-09\n",
    "        RELEASE-2025 Available Dates: 2016-08, 2017-08, 2019-08, 2022-08\n",
    "\n",
    "        >>> list_available_dates('DP1.10098.001','HOPB')\n",
    "        ValueError: There are no data available for the data product DP1.10098.001 at the site HOPB.\n",
    "    \"\"\"\n",
    "    product_url = \"https://data.neonscience.org/api/v0/products/\" + dpid\n",
    "    response = get_api(api_url=product_url)  # add input for token?\n",
    "\n",
    "    # raise value error and print message if dpid isn't formatted as expected\n",
    "    validate_dpid(dpid)\n",
    "\n",
    "    # raise value error and print message if site is not a 4-letter character\n",
    "    site = site.upper()  # make site upper case (if it's not already)\n",
    "    validate_site_format(site)\n",
    "\n",
    "    # raise value error and print message if site is not a valid NEON site\n",
    "    validate_neon_site(site)\n",
    "\n",
    "    # check if product is active\n",
    "    if response.json()[\"data\"][\"productStatus\"] != \"ACTIVE\":\n",
    "        raise ValueError(\n",
    "            f\"NEON {dpid} is not an active data product. See https://data.neonscience.org/data-products/{dpid} for more details.\"\n",
    "        )\n",
    "\n",
    "    # get available releases & months:\n",
    "    for i in range(len(response.json()[\"data\"][\"siteCodes\"])):\n",
    "        if site in response.json()[\"data\"][\"siteCodes\"][i][\"siteCode\"]:\n",
    "            available_releases = response.json()[\"data\"][\"siteCodes\"][i][\n",
    "                \"availableReleases\"\n",
    "            ]\n",
    "\n",
    "    # display available release tags (including provisional) and dates for each tag\n",
    "    try:\n",
    "        availables_list = []\n",
    "        for entry in available_releases:\n",
    "            release = entry[\"release\"]\n",
    "            available_months_str = \", \".join(entry[\"availableMonths\"])\n",
    "            available_months = [x.strip() for x in available_months_str.split(',')]\n",
    "            for available_month in available_months:\n",
    "                availables_list.append({'status':release,'date':available_month})\n",
    "        available_df = pd.DataFrame(availables_list)\n",
    "        return(available_df)\n",
    "    except UnboundLocalError:\n",
    "        # if the available_releases variable doesn't exist, this error will show up:\n",
    "        # UnboundLocalError: local variable 'available_releases' referenced before assignment\n",
    "        raise ValueError(\n",
    "            f\"There are no NEON data available for the data product {dpid} at the site {site}.\"\n",
    "        )\n",
    "    \n",
    "def nu_aop_file_names(\n",
    "\tdpid:str,\n",
    "\tsite:str,\n",
    "\tyear:int,\n",
    "\ttoken=\"\",\n",
    "\tinclude_provisional=True,\n",
    "\tcheck_size=False,\n",
    "\tsavepath=None) -> List:\n",
    "\t\"\"\"returns names of files in the specified AOP data product, site, and year. \n",
    "\t   changes the standard NEON paths to the local paths if savepath specified. \"\"\"\n",
    "\n",
    "\t# raise value error and print message if dpid isn't formatted as expected\n",
    "\tvalidate_dpid(dpid)\n",
    "\n",
    "\t# raise value error and print message if dpid isn't formatted as expected\n",
    "\tvalidate_aop_dpid(dpid)\n",
    "\n",
    "\t# raise value error and print message if field spectra data are attempted\n",
    "\tcheck_field_spectra_dpid(dpid)\n",
    "\n",
    "\t# raise value error and print message if site is not a 4-letter character\n",
    "\tsite = site.upper()  # make site upper case (if it's not already)\n",
    "\tvalidate_site_format(site)\n",
    "\n",
    "\t# raise value error and print message if site is not a valid NEON site\n",
    "\tvalidate_neon_site(site)\n",
    "\n",
    "\t# raise value error and print message if year input is not valid\n",
    "\tyear = str(year)  # cast year to string (if it's not already)\n",
    "\tvalidate_year(year)\n",
    "\n",
    "\t# if token is an empty string, set to None\n",
    "\tif token == \"\":\n",
    "\t\ttoken = None\n",
    "\n",
    "\t# query the products endpoint for the product requested\n",
    "\tresponse = get_api(\"https://data.neonscience.org/api/v0/products/\" + dpid, token)\n",
    "\n",
    "\t# exit function if response is None (eg. if no internet connection)\n",
    "\tif response is None:\n",
    "\t\tlogging.info(\"No response from NEON API. Check internet connection\")\n",
    "\n",
    "\t# check that token was used\n",
    "\tif token and \"x-ratelimit-limit\" in response.headers:\n",
    "\t\tcheck_token(response)\n",
    "\t\t# if response.headers['x-ratelimit-limit'] == '200':\n",
    "\t\t#     print('API token was not recognized. Public rate limit applied.\\n')\n",
    "\n",
    "\t# get the request response dictionary\n",
    "\tresponse_dict = response.json()\n",
    "\n",
    "\t# error message if dpid is not an AOP data product\n",
    "\tcheck_aop_dpid(response_dict, dpid)\n",
    "\n",
    "\t# replace collocated site with the AOP site name it's published under\n",
    "\tsite = get_shared_flights(site)\n",
    "\n",
    "\t# get the urls for months with data available, and subset to site & year\n",
    "\tsite_year_urls = get_site_year_urls(response_dict, site, year)\n",
    "\n",
    "\t# error message if nothing is available\n",
    "\tif len(site_year_urls) == 0:\n",
    "\t\tlogging.info(\n",
    "\t\t\tf\"There are no NEON {dpid} data available at the site {site} in {year}.\\nTo display available dates for a given data product and site, use the function list_available_dates().\"\n",
    "\t\t)\n",
    "\t\t# print(\"There are no data available at the selected site and year.\")\n",
    "\n",
    "\t# get file url dataframe for the available month urls\n",
    "\tfile_url_df, releases = get_file_urls(site_year_urls, token=token)\n",
    "\n",
    "\t# get the number of files in the dataframe, if there are no files to download, return\n",
    "\tif len(file_url_df) == 0:\n",
    "\t\t# print(\"No data files found.\")\n",
    "\t\tlogging.info(\"No NEON data files found.\")\n",
    "\t\t# return\n",
    "\n",
    "\t# NOTE: provisional filtering has been silenced for now. \n",
    "\t# if 'PROVISIONAL' in releases and not include_provisional:\n",
    "\t# if include_provisional:\n",
    "\t# \t# log provisional included message\n",
    "\t# \t# logging.info(\n",
    "\t# \t# \t\"Provisional NEON data are included. To exclude provisional data, use input parameter include_provisional=False.\"\n",
    "\t# \t# )\n",
    "\t# else:\n",
    "\t# \t# log provisional not included message and filter to the released data\n",
    "\t# \t# logging.info(\n",
    "\t# \t#     \"Provisional data are not included. To download provisional data, use input parameter include_provisional=True.\")\n",
    "\t# \tfile_url_df = file_url_df[file_url_df[\"release\"] != \"PROVISIONAL\"]\n",
    "\t# \tif len(file_url_df) == 0:\n",
    "\t# \t\tlogging.info(\n",
    "\t# \t\t\t\"NEON Provisional data are not included. To download provisional data, use input parameter include_provisional=True.\"\n",
    "\t# \t\t)\n",
    "\n",
    "\tnum_files = len(file_url_df)\n",
    "\tif num_files == 0:\n",
    "\t\tlogging.info(\n",
    "\t\t\t\"No NEON data files found. Available data may all be provisional. To download provisional data, use input parameter include_provisional=True.\"\n",
    "\t\t)\n",
    "\t\t# return\n",
    "\n",
    "\t# get the total size of all the files found\n",
    "\tdownload_size_bytes = file_url_df[\"size\"].sum()\n",
    "\t# print(f'download size, bytes: {download_size_bytes}')\n",
    "\tdownload_size = convert_byte_size(download_size_bytes)\n",
    "\t# print(f'download size: {download_size}')\n",
    "\n",
    "\t# report data download size and ask user if they want to proceed\n",
    "\tif check_size:\n",
    "\t\tif (\n",
    "\t\t\tinput(\n",
    "\t\t\t\tf\"Continuing will download {num_files} NEON data files totaling approximately {download_size}. Do you want to proceed? (y/n) \"\n",
    "\t\t\t)\n",
    "\t\t\t.strip()\n",
    "\t\t\t.lower()\n",
    "\t\t\t!= \"y\"\n",
    "\t\t):  # lower or upper case 'y' will work\n",
    "\t\t\tprint(\"Download halted.\")\n",
    "\t\t\t# return\n",
    "\n",
    "\t# Make the list of files as they should appear on the local file system and return \n",
    "\tfiles = list(file_url_df[\"url\"])\n",
    "\tif savepath is not None:\n",
    "\t\tfiles = [f\"{savepath.rstrip('/')}/{fi.lstrip('https://storage.googleapis.com')}\" for fi in files]\n",
    "\treturn files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the NEON veg data files through to gdfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpids = ['DELA','LENO','TALL','BONA','DEJU','HEAL','SRER','SJER','SOAP',\n",
    "              'TEAK','CPER','NIWO','RMNP','DSNY','OSBS','JERC','PUUM','KONZ',\n",
    "              'UKFS','SERC','HARV','UNDE','BART','JORN','DCFS','NOGP','WOOD',\n",
    "              'GUAN','LAJA','GRSM','ORNL','CLBJ','MOAB','ONAQ','MLBS',\n",
    "              'SCBI','ABBY','WREF','TREE','YELL'] # STEI, BLAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "veg_map converted to gdf with crs EPSG:32616\n",
      "Finished 0 - DELA in 14.140880823135376\n",
      "veg_map converted to gdf with crs EPSG:32616\n",
      "Finished 1 - LENO in 77.90182590484619\n",
      "veg_map converted to gdf with crs EPSG:32616\n",
      "Finished 2 - TALL in 68.52302813529968\n",
      "veg_map converted to gdf with crs EPSG:32606\n",
      "Finished 3 - BONA in 96.52222847938538\n",
      "veg_map converted to gdf with crs EPSG:32606\n",
      "Finished 4 - DEJU in 99.75511837005615\n",
      "veg_map converted to gdf with crs EPSG:32606\n",
      "Finished 5 - HEAL in 37.592040061950684\n",
      "veg_map converted to gdf with crs EPSG:32612\n",
      "Finished 6 - SRER in 92.83074045181274\n",
      "veg_map converted to gdf with crs EPSG:32611\n",
      "Finished 7 - SJER in 48.131081342697144\n",
      "veg_map converted to gdf with crs EPSG:32611\n",
      "Finished 8 - SOAP in 95.01211476325989\n",
      "veg_map converted to gdf with crs EPSG:32611\n",
      "Finished 9 - TEAK in 87.75842714309692\n",
      "veg_map converted to gdf with crs EPSG:32613\n",
      "Finished 10 - CPER in 18.05708336830139\n",
      "veg_map converted to gdf with crs EPSG:32613\n",
      "Finished 11 - NIWO in 67.67951846122742\n",
      "veg_map converted to gdf with crs EPSG:32613\n",
      "Finished 12 - RMNP in 51.67364454269409\n",
      "veg_map converted to gdf with crs EPSG:32617\n",
      "Finished 13 - DSNY in 43.67591714859009\n",
      "veg_map converted to gdf with crs EPSG:32617\n",
      "Finished 14 - OSBS in 78.17770028114319\n",
      "veg_map converted to gdf with crs EPSG:32616\n",
      "Finished 15 - JERC in 50.46046471595764\n",
      "veg_map converted to gdf with crs EPSG:32605\n",
      "Finished 16 - PUUM in 96.91682171821594\n",
      "veg_map converted to gdf with crs EPSG:32614\n",
      "Finished 17 - KONZ in 60.285563230514526\n",
      "veg_map converted to gdf with crs EPSG:32615\n",
      "Finished 18 - UKFS in 90.71670913696289\n",
      "veg_map converted to gdf with crs EPSG:32618\n",
      "Finished 19 - SERC in 90.25002384185791\n",
      "veg_map converted to gdf with crs EPSG:32618\n",
      "Finished 20 - HARV in 134.83268070220947\n",
      "veg_map converted to gdf with crs EPSG:32616\n",
      "Finished 21 - UNDE in 100.16935110092163\n",
      "veg_map converted to gdf with crs EPSG:32619\n",
      "Finished 22 - BART in 143.953204870224\n",
      "veg_map converted to gdf with crs EPSG:32613\n",
      "Finished 23 - JORN in 67.9760537147522\n",
      "veg_map converted to gdf with crs EPSG:32614\n",
      "Finished 24 - DCFS in 16.76531457901001\n",
      "veg_map converted to gdf with crs EPSG:32614\n",
      "Finished 25 - NOGP in 4.99397873878479\n",
      "veg_map converted to gdf with crs EPSG:32614\n",
      "Finished 26 - WOOD in 15.553691864013672\n",
      "veg_map converted to gdf with crs EPSG:32619\n",
      "Finished 27 - GUAN in 91.29729342460632\n",
      "veg_map converted to gdf with crs EPSG:32619\n",
      "Finished 28 - LAJA in 3.7071900367736816\n",
      "veg_map converted to gdf with crs EPSG:32617\n",
      "Finished 29 - GRSM in 60.67712950706482\n",
      "veg_map converted to gdf with crs EPSG:32616\n",
      "Finished 30 - ORNL in 65.46825194358826\n",
      "veg_map converted to gdf with crs EPSG:32614\n",
      "Finished 31 - CLBJ in 82.34106397628784\n",
      "veg_map converted to gdf with crs EPSG:32612\n",
      "Finished 32 - MOAB in 65.89712381362915\n",
      "veg_map converted to gdf with crs EPSG:32612\n",
      "Finished 33 - ONAQ in 95.13162517547607\n",
      "veg_map converted to gdf with crs EPSG:32617\n",
      "Finished 34 - MLBS in 36.34723377227783\n",
      "veg_map converted to gdf with crs EPSG:32617\n",
      "Finished 35 - SCBI in 94.84185647964478\n",
      "veg_map converted to gdf with crs EPSG:32610\n",
      "Finished 36 - ABBY in 83.57069039344788\n",
      "veg_map converted to gdf with crs EPSG:32610\n",
      "Finished 37 - WREF in 93.49670743942261\n",
      "veg_map converted to gdf with crs EPSG:32616\n",
      "Finished 38 - TREE in 104.71209764480591\n",
      "veg_map converted to gdf with crs EPSG:32612\n",
      "Finished 39 - YELL in 78.27004504203796\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Set up a session with retries\n",
    "session = requests.Session()\n",
    "retries = Retry(\n",
    "    total=5,                # Total retry attempts\n",
    "    backoff_factor=0.5,     # Wait time between retries: 0.5, 1, 2, 4, etc.\n",
    "    status_forcelist=[500, 502, 503, 504],  # Retry on these HTTP status codes\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "skipped_list = ['BONA','SOAP','TEAK','NIWO','BLAN','STEI'] # BLAN and STEI has UTM zone issues! 2 different zones. Rerun MLBS too!\n",
    "\n",
    "rerun = ['BONA','SOAP','TEAK','NIWO','MLBS']\n",
    "\n",
    "site_name = site_names[0]\n",
    "\n",
    "succeeded_list = []\n",
    "\n",
    "# for i, site_name in enumerate(site_names):\n",
    "for i, site_name in enumerate(site_names):\n",
    "\n",
    "\n",
    "\tif i <= -1:\n",
    "\t\tcontinue\n",
    "\n",
    "\tstart_timer = time.time()\n",
    "\n",
    "\t# Read the site pickle\n",
    "\twith open('/data/chloris/NEON/DP1.10098/' + site_name + '.pkl', 'rb') as f:\n",
    "\t\tveg_dict = pickle.load(f)\n",
    "\n",
    "\t# Make the veg gdf \n",
    "\tveg_gdf = make_veg_gdf(veg_dict)\n",
    "\n",
    "\t# Write that out \n",
    "\tveg_gdf.to_parquet('/data/chloris/NEON/VST/' + site_name + '_all.parquet')\n",
    "\n",
    "\t# Make the filtered veg gdf\n",
    "\ttree_gdf = filter_veg_gdf(veg_gdf,only_most_recent=False)\n",
    "\n",
    "\t# Write that out\n",
    "\ttree_gdf.to_parquet('/data/chloris/NEON/VST/' + site_name + '_single_bole_trees.parquet')\n",
    "\n",
    "\tsucceeded_list.append(site_name)\n",
    "\tprint(f\"Finished {i} - {site_name} in {time.time() - start_timer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STEI'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_name\n",
    "\n",
    "# # Read the site pickle\n",
    "# with open('/data/chloris/NEON/DP1.10098/' + site_name + '.pkl', 'rb') as f:\n",
    "# \tveg_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "veg_map converted to gdf with crs EPSG:32605\n"
     ]
    }
   ],
   "source": [
    "# Pull out the \"mapping and tagging\" dataframe\n",
    "veg_map_all = veg_dict[\"vst_mappingandtagging\"]\n",
    "\n",
    "# Filter out points that have no pointID and reindex.\n",
    "veg_map = veg_map_all.loc[veg_map_all[\"pointID\"] != \"\"]\n",
    "veg_map = veg_map.reindex()\n",
    "\n",
    "# Create a unique identifier for each point\n",
    "veg_map[\"points\"] = veg_map[\"namedLocation\"] + \".\" + veg_map[\"pointID\"]\n",
    "\n",
    "# Make a list of the unique points \n",
    "veg_points = list(set(list(veg_map[\"points\"])))\n",
    "\n",
    "# Loop every point, pull the spatial ref info, and store in lists\n",
    "valid_points = []\n",
    "easting = []\n",
    "northing = []\n",
    "coord_uncertainty = []\n",
    "elev_uncertainty = []\n",
    "utm_zone = []\n",
    "for i in veg_points:\n",
    "\t# vres = requests.get(\"https://data.neonscience.org/api/v0/locations/\"+i)\n",
    "\tvres = session.get(\"https://data.neonscience.org/api/v0/locations/\"+i)\n",
    "\tvres_json = vres.json()\n",
    "\tif not vres_json.get(\"data\"):\n",
    "\t\tcontinue\n",
    "\tvalid_points.append(i)\n",
    "\teasting.append(vres_json[\"data\"][\"locationUtmEasting\"])\n",
    "\tnorthing.append(vres_json[\"data\"][\"locationUtmNorthing\"])\n",
    "\tprops = pd.DataFrame.from_dict(vres_json[\"data\"][\"locationProperties\"])\n",
    "\tcu = props.loc[props[\"locationPropertyName\"]==\"Value for Coordinate uncertainty\"][\"locationPropertyValue\"]\n",
    "\tif cu.empty:\n",
    "\t\t# cu = pd.Series([np.nan])\n",
    "\t\tcu = np.nan\n",
    "\telse: \n",
    "\t\tcu = cu[cu.index[0]]\n",
    "\tcoord_uncertainty.append(cu)\t\n",
    "\teu = props.loc[props[\"locationPropertyName\"]==\"Value for Elevation uncertainty\"][\"locationPropertyValue\"]\n",
    "\tif eu.empty:\n",
    "\t\t# eu = pd.Series([np.nan])\n",
    "\t\teu = np.nan\n",
    "\telse:\n",
    "\t\teu = eu[eu.index[0]]\n",
    "\telev_uncertainty.append(eu)\n",
    "\tutm_zone.append(32600+int(vres_json[\"data\"][\"locationUtmZone\"]))\n",
    "\n",
    "# Create a dataframe with the spatial info of the reference points\n",
    "pt_dict = dict(points=valid_points, \n",
    "easting=easting,\n",
    "northing=northing,\n",
    "coordinateUncertainty=coord_uncertainty,\n",
    "elevationUncertainty=elev_uncertainty,\n",
    "utm_zone = utm_zone)\n",
    "\n",
    "pt_df = pd.DataFrame.from_dict(pt_dict)\n",
    "pt_df.set_index(\"points\", inplace=True)\n",
    "\n",
    "# Add the reference info to the veg map\n",
    "veg_map = veg_map.join(pt_df, \n",
    "on=\"points\", \n",
    "how=\"inner\")\n",
    "\n",
    "# Compute the Easting of each stem \n",
    "veg_map[\"stemEasting\"] = (veg_map[\"easting\"]\n",
    "+ veg_map[\"stemDistance\"]\n",
    "* np.sin(veg_map[\"stemAzimuth\"]\n",
    "* np.pi / 180))\n",
    "\n",
    "# Compute the Northing of each stem\n",
    "veg_map[\"stemNorthing\"] = (veg_map[\"northing\"]\n",
    "+ veg_map[\"stemDistance\"]\n",
    "* np.cos(veg_map[\"stemAzimuth\"]\n",
    "* np.pi / 180))\n",
    "\n",
    "# Compute the stem uncertainties\n",
    "veg_map[\"stemCoordinateUncertainty\"] = veg_map[\"coordinateUncertainty\"] + 0.6\n",
    "veg_map[\"stemElevationUncertainty\"] = veg_map[\"elevationUncertainty\"] + 1.5\n",
    "\n",
    "# Test that all points are in the same UTM zone\n",
    "if len(set(veg_map[\"utm_zone\"])) != 1:\n",
    "\traise ValueError(\"Points in the veg map are in different UTM zones! Rectify before proceeding.\")\t\n",
    "else:\n",
    "\t# Define the crs\n",
    "\tcrs = f\"EPSG:{veg_map['utm_zone'].values[0]}\"\n",
    "\tprint(f\"veg_map converted to gdf with crs {crs}\")\n",
    "\n",
    "\t# Create list of shapely points for the gdf \t\n",
    "\tgeometry = [Point(xy) for xy in zip(veg_map[\"stemEasting\"], veg_map[\"stemNorthing\"])]\n",
    "\n",
    "\t# Convert veg_map to a gdf \n",
    "\tveg_map = gpd.GeoDataFrame(veg_map, crs=crs, geometry=geometry)\n",
    "\n",
    "\tveg_dict[\"vst_apparentindividual\"].set_index(\"individualID\", inplace=True)\n",
    "\tveg_gdf = veg_map.join(veg_dict[\"vst_apparentindividual\"],\n",
    "\ton=\"individualID\",\n",
    "\thow=\"inner\",\n",
    "\tlsuffix=\"_MAT\",\n",
    "\trsuffix=\"_AI\")\n",
    "\n",
    "\t# return veg_gdf\n",
    "\tveg_gdf.to_parquet('/data/chloris/NEON/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 911,  912,  913,  914,  915,  916,  917,  918,  919,  920,  921,\n",
       "         922,  923,  924,  925,  926,  927,  928,  929,  930,  931,  932,\n",
       "         933,  934,  935,  936,  937,  938,  939,  940,  941,  942,  943,\n",
       "         944,  945,  946,  947,  948,  949,  950,  951,  952,  953,  954,\n",
       "         955,  956,  957,  958,  959,  960,  961,  962,  963,  964,  965,\n",
       "         966,  967,  968,  969,  970,  971,  972,  973,  991,  992,  993,\n",
       "         994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003, 1004,\n",
       "        1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015]),)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(veg_gdf['coordinateUncertainty'].isna())\n",
    "# veg_gdf.to_parquet('/data/chloris/NEON/test.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chloris-agbd-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
